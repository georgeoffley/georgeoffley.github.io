var hostname = "https://georgeoffley.com";
var index = lunr(function () {
    this.field('title')
    this.field('content', {boost: 10})
    this.field('category')
    this.field('tags')
    this.ref('id')
});



    index.add({
      title: "Dockerizing System Tests With Selenium",
      category: ["Blog"],
      content: "\n    \n\n\nTable Of Contents\n\n  Introduction\n  Selenium WebDriver\n  Setup\n  Running Tests\n  Notes\n  Conclusion\n\n\nIntroduction \n\nWe are spinning up a new application for some end users to enter data, so we need to build a robust testing system. Unit tests are a must, but those only test to ensure that all the classes, methods, and functions do what we expect. On top of that, we also need to verify that the web app as a whole looks and behaves how we hope it does to have a complete end-to-end testing apparatus. My goal for these first few spikes in the project was to find a tool for system testing and see if we could make it modular and easily automate it to not interfere with our workflow. I believe I found the solution. Selenium is a suite of tools for creating and automating browser tests, and I think it is what I was looking for. Let’s dive in.\n\n\n\nSelenium WebDriver \n\nSelenium is a multi-language suite of tools for creating browser-based automation tests. This tool will automate opening a browser, looking for a test condition, and reporting a pass or fail. Selenium is a whole suite of tools, but I decided to focus on Selenium Web Driver.\n\nMuch like in the name, the software “drives” the web browser. Much like humans, we can bring up the webpage in a browser, enter in data, look up tags, and utilize all these capabilities to write tests using the API Selenium provides. A test opens a browser, and like magic, many things happen.\n\nMy goal is to automate as much of this process as possible. So it would not be ideal to have a bunch of browsers launch with with ghosts running tests. A better solution was to run headless browsers using container images provided by Selenium. This solution gives us a container with a built-in web browser which we don’t see that our script runs a test against. The results we can see in the terminal.\n\nAs our project utilizes Docker, the easiest way to start is to grab the Docker image for the browser and start making some scripts. How this will look in the final solution is still to be determined, but I can replicate what I did for the sake of this write-up.\n\nSetup \n\nTo begin, I set up a docker-compose.yml file with the following:\n\n\n\nThis gave us two containers, our browser using the Chrome-based selenium image and a Ruby-based container to run the script. Our new stack uses a lot of Ruby. The system container is built using an image from the docker file in the test directory.\n\n\n\nThis build handles installing the gems we need, including selenium-webdriver and the chromedriver-helper. Both are used to run our Web Driver script and utilize Chrome capabilities.\n\n\n\nFinally, the last bit is the script itself.\n\n\n\nRunning Tests \n\nLet’s look at this script. My Ruby is rusty, but I tried my best. We set all our requirements, set a timer to make the thread sleep (more on that later), and then we write our test. In the script, we are writing the tests using the RSpec. RSpec is a domain-specific language built using Ruby to test Ruby code. We use this to test our behaviors using the describe and it blocks.\n\nWe start by defining the capabilities we are looking for; in this case, we are testing a chrome browser, so we need to specify that.\n\nThen we use a variant of WebDriver called the Remote WebDriver. Remote Driver is written identically to WebDriver, just with the caveat of the driver logic looking for the browser in another system. Here we set the address for the Chrome Selenium container so that our WebDriver knows to look for this remote machine to run the test against.\n\nBoth containers are in the same network provided by Docker Compose, so we use the hostnames. Also, note that we are mapping port 4444 as the WebDriver will use this port to communicate.\n\nWe then set the driver to navigate to our chosen website as an action. The following line sets what we expect to see using RSpec’s handy expect function. We expect the page’s title to be equal to a string we provide and fail if the title is mismatched. Then we’ll just take a screenshot and save the image to a local drive using one of the built-in save_screenshot functions. Finally, and this was important, use the quit function.\n\nWe run this just by running docker-compose up, and we can see the test passed as going to Google does indeed yield a page title of “Google.”\n\n\n\nWe can also see the screenshot taken from the remote browser.\n\n\n\nNotes \n\nUsing the quit function was essential to kill the connection, but it also closed the browser we can’t see. Additionally, I mentioned I would come back to using the sleep function in Ruby. It turns out using the depends_on feature in Docker Compose is not enough to ensure services are available to each other. When I began running the network, the remote driver would continually fail to connect. It turns out we just needed a moment for everything to boot. Docker recommends creating a wait script but pausing the thread for a moment worked just as well.\n\nConclusion \n\nThis was a pretty simple example, but it answered my questions. We can use this to test website behavior, make it modular, and automate the tests. That checks enough boxes for me to keep going down this rabbit hole. The next goal is to develop an automated solution and possibly clean up the deployment a little. I’m thinking of having these two containers not run with the rest of the project and boot up the containers using a bash script specifically for testing. I might write about that too.\n\n-George\n",
      tags: ["Docker","Testing","Intermediate"],
      id: 0
    });
    

    index.add({
      title: "Shared Memory in Docker",
      category: ["Blog"],
      content: "\n    \n\n\nTable Of Contents\n\n  Introduction\n  Shared Memory Device\n  Use in Docker\n  Conclusion\n\n\nIntroduction \n\nRecently, I was experimenting with system testing on a new stack. While working with Selenium Remote Driver to create headless web browsers (more on that later) to run against my app, I came across a Docker option I had not known. This Docker feature allowed for easily accessible and fast file reading and writing between the host and the container. It was being used as a volume, but I learned it was much more. When I looked it up, it was referred to as a Shared Memory Device.\n\n\n\nShared Memory Device \n\nTurns out that this is an old Linux staple. The shared memory device, located in the /dev/shm/ directory in the file system, utilizes temporary storage using RAM rather than disk storage. Using a shared volume available on the RAM and not the disk makes for a much faster read/write speeds. These devices also allows for inter-process communication.\n\nUse in Docker \n\nDocker allows for the use of this device. When conducting my testing, I used the volume mapping feature to easily map local directories with directories inside the containers we’re working on. As seen below.\n\ndocker run --rm -it -v /dev/shm/:/dev/shm/ --name ubuntu ubuntu\n\n\nThis mapped the local shared memory device to the Docker device. Docker, by default, allows for up to 64 MB of storage space using the shared memory device. You can see the default allocation when searching for the shared storage device within the container. As seen below.\n\n$ docker inspect ubuntu | grep -i shm\n            \"ShmSize\": 67108864,\n\n\nI was saving screenshots just to ensure that I was looking at the thing I was hoping to look at. Although I was only doing some foundational work, I could find myself in a pickle with only 64 MB of space available if this work ends up scaling. Should we need to, Docker lets us change the size of this device using the –shm-size option. As seen below.\n\ndocker run --rm -it -v /dev/shm/:/dev/shm --shm-size=2gb --name ubuntu ubuntu\n\n\nAbove we add in the –shm-size option and we can specify all the space we need! Let’s look one more time to confirm that it works.\n\n$ docker inspect ubuntu | grep -i shm\n            \"ShmSize\": 2147483648,\n\n\nShazam! We have more storage in our shared memory device to play around with.\n\nConclusion \n\nThere is more to come about my adventures in Dockerizing system testing, but for now, I was able to learn a new thing about Linux and Docker. I hope it helps someone who came across it without knowing as I did.\n\n-George\n",
      tags: ["Docker","Intermediate"],
      id: 1
    });
    

    index.add({
      title: "Using Amazon API Gateway with Lambda",
      category: ["Blog"],
      content: "\n    \n\n\nTable Of Contents\n\n  Introduction\n  Amazon API Gateway\n  Setting up an API\n  Adding the Trigger\n  Setting the Methods\n  Conclusion\n\n\nIntroductions \n\nI follow this movie podcast where they review and talk about well movies. It’s a fantastic podcast with a diverse set of people who’ve worked in the industry a long time, and I enjoy their opinions. At the end of each episode, they do a “Staff Pick” where they pick out a movie that shares themes with the movie they just reviewed or movies they think people should see. As I don’t have time to watch all the movies they suggest every week, I need to keep a running list of these films. They offer some good stuff.\n\n\n\nThis presented a good opportunity to get some practice using React. I could build a tiny site cataloging all the suggested films. I have no React knowledge, so I started by creating a database to store their suggested films to stick with the iterative approach. Now I need to set up something to query the database and get back a list of the movies. That is where Amazon API Gateway and Lambda come in.\n\nAmazon API Gateway \n\nI’ve covered AWS Lambda before, so I won’t go into it much. Amazon API Gateway, however, is an AWS service that allows you to quickly spin up API endpoints for use in your web applications. When you need to request data for your website, an API endpoint is what you’ll use. An API endpoint is a URL that your application can reach out to request some data. For example, this API endpoint lets me look up Studio Ghibli movies.\n\nThese can scale like crazy, and API Gateway lets you manage, monitor, and add endpoints to your service quickly. For this project I am spinning up, we only need to return a list of the movies in the table. So let’s begin.\n\nSetting up an API \n\nThe first thing is setting up something to process the request once our service reaches out via an API endpoint. AWS Lambda is what we’ll use to create the process to query our database and return all our stuff.\n\n  So we go into the AWS Console and create our Lambda. We’re returning all our items with a simple query, so it’s an easy setup. Here’s the code:\n\n\nimport boto3\nimport os\n\ndef lambda_handler(event, context):\n    client = boto3.client(\n        'dynamodb',\n        aws_access_key_id=os.environ.get('KEY'),\n        aws_secret_access_key=os.environ.get('SECRET')\n        )\n\n    def get_all_items():\n        \n        response = client.scan(\n            TableName='MaxFilmStaffPicks',\n        )\n\n        return response\n        \n        \n    return get_all_items()\n\n\nAdding the Trigger \n\n\n  After we have the Lambda function up and running, we need to create something to initiate it, like a trigger.\n  You can see a button to Add trigger at the top of the Lambda menu. You can click that and select API Gateway in the proceeding menu.\n\n\n\n\n\n  Then you can hit Create API, which brings up the options for APIs. In this case, I’m making a REST API, so I select that.\n  For security, I pick Open, as this is just a practice app. You can see what I chose below.\n\n\n\n\nSetting the Methods \n\n\n  I finish off by going into the API Gateway menu to check out the newly created endpoint.\n  After that, you’ll be greeted by a screen with all our methods. The default created method is the ANY method; I want this to be a GET method. The difference between HTTP messages is a bit out of scope, but you should know them. We are using this endpoint only to request information for our application, so we stick to GET.\n  We’ll select the ANY method, click the Action menu above it, and delete the method.\n  Then we go back to the same menu and click Create Method.\n  From the dropdown, we select GET.\n  Following this, we enter the name of the Lambda function this is for, and we’re done.\n\n\n\n\nVery last step is to deploy the API.\n\n\n  We click the same menu and click Deploy API.\n  You’ll then be asked which Stage you want to deploy in. When creating APIs, you can create many stages for your APIs, which will affect how they are used—for example, setting up a dev stage or creating different versions of your API. Whatever stage you set will reflect in the endpoint. See below.\n\n\n\n\n\n  For this instance, it is one API endpoint, in one app, for one specific purpose. I just used the default.\n  Now we’re done. We test the endpoint, and we get data!\n\n\n\n\nConclusion \n\nNow I have a working API for grabbing my data. React confuses me, as I thought I’d be able just to pull data like I would a Python app. However, I am sure I will learn much more as I continue. And now you also have some knowledge of setting up endpoints in Amazon API Gateway.\n",
      tags: ["AWS","Lambda","API Gateway"],
      id: 2
    });
    

    index.add({
      title: "Shenanigans with Shaders",
      category: ["Blog"],
      content: "\n    \n\n\nTable Of Contents\n\n  Introduction\n  Shaders\n  Setup\n  Shader Code\n  Conclusion\n\n\nFor those of you who love rabbit holes, learning graphics programming is a pretty deep one. There’s always some new thing to learn, there’s a bunch of different new languages and toolsets to know, and on top of all that, there’s math. Like anything else in programming, you pick up momentum with each new thing you build, so I found a tutorial and started making shaders. I know very little about this. However, I’m writing what I’m learning, so don’t come for me if I’m off on anything.\n\n\n\nShaders \n\nA shader is a program that runs on the GPU as part of the graphics pipeline. We’re going to focus primarily on shaders in Unity. There are other ways to tackle this, but Unity gives an easy setup to get started quickly. For the context of Unity, a shader is a small script containing logic and calculations for determining the colors of a pixel.\n\nIn Unity, we create shader objects which act as wrappers for our shader program. A shader object exists in a shader asset which is just the script we are writing. Creating these in Unity allows for a great deal of freedom in what we make. What we’ll focus on is adding some basic functionality to a shader. We’ll be focusing on using ShaderLab to create shaders.\n\nSetup \n\nThe first thing to set yourself up making shaders in Unity is Unity. So download it, and create a new project.\n\n\n\nI won’t give a full rundown of Unity and the stuff you can do. I leave that to better minds. In the Hierarchy Window, right-click and scroll to 3D Object and click whichever object grabs your fancy. I always pick sphere for testing stuff. Now we have a 3D Mesh on the screen that we can begin adding things to it. In the Project Window, right-click on the word Assets and create two new folders, Materials and Shaders. Double click into the Materials folder, right-click and Create is right at the top -&gt; click Material. Materials are similar to skins we can apply to 3D objects. We will use this new material to add our new shader to the 3D Mesh. After that, drag our new material into the Scene Window where our sphere is and onto the sphere we made. Now right-click our Shaders folder scroll to Create -&gt; Shader -&gt; Standard Surface Shader. Click the sphere in the Scene window to bring up the Inspector Window. Finally, drag the shader file over to the inspector window with our sphere covered in our new material. We have just applied our shader to the materials. You should see this in the Inspector Window.\n\n\n\nNow go back to the Project window and double click our new Shader file. Unity will launch an IDE for use to check out the code. You can configure your choice of IDE; I have VSCode configured. Open the Shader file, and let’s check out the code. I created some basic shader code you can use.\n\nShader Code \n\nHere is the complete, minimal shader code:\n\n\n\nIt looks a bit much to anyone new to this, including myself, so let’s take it a section at a time. The first thing at the top, starting with “Shader,” is the Shader Block. This is used to define our Shader Object. You can use this to define your properties, create many shaders using the SubShader blocks, assign custom options, and assign a fallback shader object. Here you can see the name of our shader and that it is in the “Custom” directory.\n\nWithin the Shader block curly brackets, we have our other sections. The first is our Properties. The properties box is where we define the properties for our materials. A material property is what Unity stores along with our materials. This allows for different configurations within Unity by creating things like sliders and inputs within the Inspector window for us to play around with. We defined two properties, the MainColor and the MainTexture. Using square brackets, I outlined which property was the default color and default texture. We also defined the default values for these properties. There’s a bit to these values but suffice it to say, both values are default white.\n\nThe second block is our SubShader; this is where our shader logic goes. You can define multiple sub shaders for many different uses. For example, depending on the graphics hardware you want to support, you can make shaders for the various graphics APIs. Within our block, you can see some code for assigning tags, assigning levels of detail (LOD), and the CGPROGRAM block. I want to draw your attention to this section of the code:\n\n\n\nFirst, we define the data types for our inputs and outputs and create a function for us to serve the outputs into unity. Our Input we set up as uv_Maintex; this allows for us to input a texture object. Then we create a fixed4 variable for our _Color attribute. The o.Albedo parameter is what is used to control the base color of the surface. Here we are taking the values of our texture and multiplying them by our color input. The code above gets you something similar to this:\n\n\n\nI was proud of myself the first time I made this from memory. Our coded shader lets us control the color of the material and add basic textures to it. Working in graphics does not lead to instant gratification, as anything you do requires a ton of setup. However, this and ShaderToy get you that dopamine hit.\n\nConclusion \n\nAbove I went through some fundamentals of shaders in Unity. I skipped over a ton of information as I’m still learning a lot, and a full informed explainer would be twenty pages long. There is a lot to programming graphics and shaders specifically. I suggest you check out stuff like Team Dogpit’s shader tutorial for a way better deep dive. I’m excited to dig into this world. I want to learn to create some of the incredible stories I see in animation, and any first step is a step in the right direction. Thanks for reading.\n\n-George\n",
      tags: ["Graphics","Unity","ShaderLab"],
      id: 3
    });
    

    index.add({
      title: "Messaging and Madness: Sending Messages with AMQP and Amazon MQ",
      category: ["Blog"],
      content: "\n    \n\n\nTable Of Contents\n\n  Introduction\n  AMQP\n  AMQP and Amazon MQ\n  Serialization\n  Conclusion\n\n\nIntroduction \n\nHow do software systems talk to each other? Back-end systems can scale into giant melted together Cronenberg monsters, often making up different tools and languages. So, communicating between these services can become an untenable challenge without some shared vocabulary. We can communicate in many ways, but today I wanted to talk about asynchronous messaging protocols and figure out how AWS can help.\n\n\n\nAMQP \n\nAMQP stands for Advanced Message Queuing Protocol. I’ve been working to implement it for some back-end software suites I’m building out to enable them to talk to each other. AMQP utilizes these things called brokers to publish messages on, then on the other end, a receiving service subscribed to the same “channel” that we posted to can pick up that message.\n\n\nvia Rabbit MQ Tutorials\n\nLet’s dive a little further down; the publisher service publishes a message to an exchange on a broker. This exchange has routes that lead to queues, or “channels,” where the payload is published. We make sure to include the sending information with our message to be routed to the correct queue. The broker cannot see the message, although it might look into any metadata attached to the message from the publisher. This workflow asynchronously sends messages. Imagine a server version of a mail sorting machine shooting letters into the correct mail slot based on the address.\n\n\n\nWhen referring to a publisher, I mean some code that we utilize to connect and send a message. AMQP is programmable, so I can shape it to fit most situations. In this case, we need to send messages to our different software suites to trigger actions to happen. Learning this took some time, but it’s been simple to implement.\n\nThere are different types of exchanges that we can use to make these services fit our needs. I’m going to explain what we use briefly.\n\nWe use a direct exchange utilizing routing keys to bind queues to exchanges. Our code can use direct exchanges to distribute tasks to many different endpoints, but we used these direct exchanges to make direct routes between our services. Other types of exchanges can be used to broadcast messages. More information can be found here. For now, we’re going to focus on direct exchanges.\n\nAMQP and Amazon MQ \n\nWe touched on all that because I wanted to talk about Amazon MQ. Amazon MQ is a fully managed platform for setting up message brokers. Amazon MQ utilizes both RabbitMQ and Apache Active MQ for creating brokers. We’re sticking with Rabbit MQ for the time being.\n\n\n\nHere above, you can see you can easily set up a broker in just a few clicks. I left most of the settings on default, except for choosing “RabbitMQ” for our broker engine and setting some security up for accessing our management console.\n\n\n\nOnce we get that, we have access to the RabbitMQ dashboard Amazon MQ created and is managing. Now that we have a broker set up, we can play with some code.\n\n\n\nAbove I use the library Kombu to create some connections and send some stuff. I started by setting up our environment variables. Then created exchange and queue objects. Finally, I made our connection object and the producer object, and then we sent a simple “Hello” message.\n\nSerialization \n\nSerialization is another blog post, but I chose to use JSON to serialize the payload. In the production software, I use a combination of JSON and Pickle to serialize things like image data.\n\nNow we can see our message published on the queue I declared in our publisher service. An identical receiving service would be set up on the other side to read out messages sent to that queue.\n\n\n\nConclusion \n\nIn conclusion, using Amazon MQ allows us to set up managed brokers for us to send messages. With AMQP as the broker engine, we have a lightweight message-sending workflow. Thanks for reading.\n\n-George\n",
      tags: ["AWS","Python","AMQP"],
      id: 4
    });
    

    index.add({
      title: "Health Checking S3 and DynamoDB in AWS",
      category: ["Blog"],
      content: "\n    \n\n\nTable Of Contents\n\n  Introduction\n  Problem\n  S3 Solution\n  DynamoDB Solution\n  Conclusion\n\n\nIntroduction \n\nA hybrid infrastructure has tons of exciting challenges. Although we host a great deal of our software in AWS at my company, we cannot do everything in the cloud. As such, we have tons of physical infrastructure as well. This hybrid infrastructure presents many challenges that we strive to overcome on the software team. One of the challenges we are working towards is imaging and utilizing software to detect our yields. This piece of that puzzle will focus on storage for our images.\n\n\n\nWe decided that we would use a combination of services offered by AWS. The first is the Amazon Simple Storage Service or S3 for image storage and DynamoDB for holding metadata of said images. Given that we are getting information straight from hardware, many things might go wrong, from getting the pictures to when said pictures are pushed to AWS. This brings us to this evening’s question: How can I be sure these services are available for me to send stuff to?\n\nProblem \n\nWell, as it turns out, there are a few ways this can be done. For example, there are libraries out there that will scan health check websites to see if AWS has any service outages. This would not be a great way to do health checks for a production application. So, I decided to spike this problem and make something myself. I am not worried about AWS services being out as they have high availability using their different availability zones. I am more concerned about our endpoints failing, internet issues, or Cloverfield monsters. So, this needs to be explored.\n\nS3 Solution \n\nA simple solution for checking the health of my resources was needed. Luckily, I quickly put something together using the Boto3 library, which is the AWS SDK for Python. This library gives us easy access to the AWS API for configuring and managing services. The first thing I did was create an object class to utilize the Client class in Boto3.\n\n\n\nWe only need to pass in our access credentials and the services we want to create a client object for, and we get our client object back. Each turn in Boto3 allows for interacting with the Client class. The docs define the Client class as “a low-level client representing whatever service”. In most cases, you would use it to access the various functions for interacting with the service.\n\nAfter that, I put together some simple logic to return some information on the resource we are looking for. In our case, we were trying to get access to a bucket where we will store images. This solution is enough to satisfy me that the resource exists, and I can communicate with it. Below is the code I used for S3.\n\n\n\nThe code above sets up a new client instance and utilizes the head_bucket() function. This is great for seeing if a bucket exists and if the entity polling it has permissions to access it. In my case, I only need to be able to see if I get a message back. So, I pass in the bucket name, and I can receive a 200 message back from the server if the resource is there and I have access to it. I like this approach because it is dead simple, and I also get to utilize the custom exception that we get access to using the client object, which is the NoSuchBucket exception. Using this exception allows us to be concise with our exceptions.\n\nThere were some questions about the limitations on being able to use something like this. We expect to use this frequently to pole S3 and make sure that we can talk to our bucket. If AWS is not available, we need to turn off the spigot and stop our software from sending stuff to AWS and not lose messages in the void of space. That said, we will be polling a few times a second at least; luckily for us, S3 upped their request rate to 3500 to add data and 5500 for retrieving data. This gives us plenty of room to be able to pole what we need.\n\nDynamoDB Solution \n\nWith the client object that we created above, we can also use that to access DynamoDB. As such, the code is below:\n\n\n\nThe above code snippet does the same thing as the S3 code does. We create a new instance, and we use the describe_table() function while passing in the table name. This function returns information about the table, including the status. Also, note that the ResourceNotFoundException is another custom exception provided by the Dynamo Client object. This bit of code satisfies what I need to be able to check the status of a table. Yay!\n\nUsing this method also has similar challenges. The decribe_table() function uses up an eventually consistent read on your table. So, getting out-of-date data is possible if you are polling something you just created, so give it a second. If you are using a provisioned table in Dynamo, this method will take up one of your reads per second. We will need to make sure this is accounted for when we start designing our database.\n\nConclusion \n\nThe above simple bit of code was a brief spike for a solution we needed to explore. This write-up was inspired by a lot of the help I received from my fellow AWS Community Builders. Checking the health and status of services is one of many things that we will build out using AWS. I am excited to keep up my learning and building. If you have seen or made other stuff to accomplish this type of work, let me know! I would love to learn more.\n",
      tags: ["AWS","Python"],
      id: 5
    });
    

    index.add({
      title: "Encrypting Your Environment Variables in Lambda with KMS",
      category: ["Blog"],
      content: "\n    \n\n\nTable Of Contents\n\n  Introduction\n  Key Management System in AWS\n  Customer Master Keys\n  KMS and Lambda\n  Conclusion\n  Additional Notes\n\n\nIntroduction \n\nDo you hate when gnomes steal your underpants for profit? I know I hate when those guys come out to steal my stuff. Unfortunately, I cannot help you prevent the theft of your undergarments, but I can help you protect some assets in AWS. Specifically, we are going to talk about encrypting environment variables in Lambda.\n\n\n\nIn a previous article, I talked through about how to create a Twitterbot using AWS Lambda. I mentioned that we would talk about encrypting environment variables in my last AWS article, and this is it. Encrypting stuff is a big topic and crucial for maintaining a secure environment and not just in Lambda.\n\nWe all know what encryption is, an ancient method for converting information into a secret code that only the correct parties would have access to. Usually, this is in the form of a key which both parties have access and can use to encrypt and decrypt. Anyone else looking at it would not be able to tell the difference between it and white noise. Records of civilization using encryption go as far back as Egyptians in 1900 B.C. using it to encode messages on the walls of tombs. Our modern solution for this ancient technique requires us to dive into yet another service that AWS offers, Key Management Services, or KMS.\n\nKey Management System in AWS \n\nKey Management service, in short, is a service that allows us to manage encryption keys for various AWS services or within your AWS applications. KWS gives you a central repository to easily create, manage, and rotate your encryption keys. If you are wondering, the act of rotating encryption keys is when you generate new keys, re-encrypt all the data using the new keys, and then delete the old keys. It is an essential service in AWS.\n\nKMS is considered a multi-tenant hardware security module or HSM. It’s a blade server sitting on a rack that handles the numerous clients managing keys in AWS every day. The hardware is created in a way that many clients can use the hardware but still be virtually isolated. The keys are stored in memory and not written to disk as a security measure. This way if the hardware is powered down the keys are gone and thus inaccessible.\n\nAWS does provide a single-tenant solution for enterprise businesses called CloudHSM. This gives more control to the client. Now, let’s have a small discussion about the captivating subject of standards in cryptography.\n\nIn 1901 the National Institute of Standards and Technology was founded as a laboratory for promoting innovation and industrial competitiveness for the science and technology sector. Now a part of the U.S. Department of Commerce, these are the folks who set security standards for the stuff we use. In 2002 the E-Government Act was signed into law and then amended in 2014 to include new measures for cybersecurity. This included several plans to beef up encryption standards, among them is the Federal Information Process Standards or FIPS. This program sets legal requirements for U.S. government systems and the systems for any contractors. The reason I bring all this is up is that KMS in its most basic form is compliant with FIPS 140-2 Level 2 compliant. If a client were to use the single-tenant CloudHSM solution for managing keys, then they comply with FIPS 140-2 Level 3. The different levels break down into levels of security through more aggressive security solutions. You can read about all the levels here. A business would only need to worry about having their own CloudHSM if they are trying to comply with a specific regulation. For most of us, a regular KSM solution would work fine.\n\nCustomer Master Keys \n\nAlright, so we know KMS is super cereal.\n\n\n\nNow how does it work? The primary resources in KMS are the Customer Master Keys or CMKs which are logical representations of the master keys. These master keys are used to encrypt and decrypt our data. They use what is called envelope encryption for securing keys and enabling encryption.\n\n\n\nWhen you encrypt your data whatever you encrypted is protected but you also need to protect the encryption keys. Envelope encryption is the practice of encrypting plaintext data with a data key and then encrypting the data key with another key. The data keys are strings of data used to unlock crypto functions like authentication, authorization, and encryption. The role of the master key is to keep the data keys safe. The master keys are what is stored on the HSM and they are used to encrypt all the other keys. The CMKs you make also have metadata attached to them which track key ID, description, creation date, and key state. This metadata also includes the needed material for encryption and decrypting data.\n\nSo, let’s see what we’re talking about.\n\n\n\nThis is the KMS dashboard where you can check out your keys. On the left, you notice that it defaults to the “Customer managed keys” menu where you can create your keys. There is also the option for AWS Managed Keys. These are keys created by AWS for various services that you use. For myself, I have a key for Lambda which is a default key I can use to encrypt environment variables. And a key for the Cloud9 IDE AWS offers. Cloud9 is an awesome cloud-based IDE where you won’t run into issues with Posix permissions for stuff you make. Mini-rant over. The last option is for any “Custom key stores” you might have. Clients using AWS CloudHSM would come here to manage the keys they control within their cluster.\n\nSo, let’s create a key\n\n\n\nAfter we hit the Create Key button we are taken to the Configure Key page. Here we can choose which type of encryption we want. Here we have a couple of options, Symmetric and Asymmetric encryption.\n\nCreating a symmetric key means we are going to create a 256-bit key that uses the same secret key to perform both the encryption and decryption processes. Something like an S4 bucket would be a great candidate for this kind of encryption which uses the AES-256 encryption standard. For me, I created a test key for encrypting environment variables using symmetric encryption.\n\nThe other option is Asymmetric encryption also known as public-key encryption. This will create an RSA key pair used for encryption and decryption or it can also be used for signing and verification, but not both. With an asymmetric key, we create a public key used for encryption and a private key used for decryption. This we can create for something like an EC2 key pair for logging in via SSH to an instance.\n\nThe advanced options let you use a custom store from a CloudHSM a client might own or import a key from an external key management infrastructure. When importing something from an external key management service AWS lays out some rules about how you can’t use KMS to change the key material and how you assume responsibility for some of the things AWS assumes responsibility for when you use KMS to create keys. More information is here.\n\n\n\nHere we can add aliases for the keys. We can also add a description. We can also add tags that we might have created for tracking these assets through our billing set up.\n\n\n\nThe next screen lets us give out permissions for our key. In a large environment, you can expect to assign this key to the needed administrator. That way only that user has permissions to administer the key using the KMS API. Another option you also get is whether to allow admins to delete this key we are making. For any environment, I am making I would probably uncheck this. From my non-cloud sys admin perspective when you have lots of keys distributed around to many different services to have a key deleted for any reason is a recipe for potential nightmare scenarios hunting down where the key was used and redeploying the data since there’s a good chance you’re locked out of getting to it.\n\n\n\nThe next screen lets us give basic permissions to use the CMK in cryptographic operations. So we can choose who can use this key to encrypt and decrypt assets. You can also add additional accounts here if we need another user that might not be on the list.\n\n\n\nFinally, we can review and make changes to the key policy. AWS creates these rules via a JSON file. So, if you know how to navigate through the fields and understand what you are changing you are free to change the policy as you see fit. I would suggest using the GUI to do anything rather than changing the JSON directly. Unless you know what, it is you are doing. Screw around and find out at your peril.\n\n\n\nKMS and Lambda \n\nO.K. we spent a bunch of time talking about a bunch of nerd stuff. I just wanted to encrypt my environment variables. What is all this? We are almost there I promise.\n\n\n\nNow that we have a CMK set up let’s go into Lambda. Maybe we go into a new function that we created to test this out. Then we go into the environment variables. Now we can talk a little about how Lambda works and what impacts there are when we encrypt environment variables. Now Lambda is essentially a service that runs a single instance when called. So, no one sees your environment vars other than when a user is in the console. That said if you are sending API requests with keys and secrets anyone who might be listening on the line might be able to see it. Which is why we do all this stuff. So, before we send API secrets to Lambda lets encrypt it.\n\n\n\nSo, when we go to make our envs we have the option of encrypting them. This will encrypt them in transit and on the console too. If you notice I already created a variable and encrypted it. Under the value, there is just a random string of gibberish to the naked eye but with the key, I can decrypt the value and get the test key value. If we add a new variable, we can check the Enable helpers for encryption in transit under the Encryption configuration section which brings up the Encrypt button. In that section, we also get the option of using the default Lambda key that AWS creates or one of the CMKs we made earlier. I mentioned this before, but Lambda has a default key that you can use for encrypting environment variables. Makes it easy to encrypt environment variables without setting up additional CMKs. I chose to use a CMK to demonstrate KMS but using the Lambda key would probably be fine if you only have a few. When you start to scale, and you need keys for tons of stuff is where I would say using CMKs is a good idea. It gives you a lot more options for tracking and auditing.\n\nAfter hitting the Encrypt button we get a pop up which gives us the execution policy in the form of a JSON readout. In addition to that, we get one of my favorite things AWS gives you, the code to access your keys. I’m lazy and I will always appreciate being given the code to decode rather than digging through the API.\n\n\n\nSo, I’m just going through how to decrypt and utilize your keys within your Lambda function. You can do a bit more with the API and I suggest you check out the reference here. Similar to when you’re looking for your unencrypted env variable you can use the os.environ() function to grab our variable. Accept now we have just a long string of gibberish. Here is where we use the boto3 library to do some stuff.\n\nBoto3 is the AWS SDK for Python. Here we look for the kms service and use the built-in client class for accessing the KMS functions. Then you can see we use the decrypt() function for decrypting the ciphertext we pass into it. The code also imports the b64decode class from the bas64 module which we use to decode the Encrypted var we grabbed. After that, we need to set our EncryptionContext which is a set of non-secret key-value pairs which represent additional authentication data. The options passed in need to match the same context that was used for encrypting the data.\n\nQuickNote\nYou should know that these are for symmetrical CMKs which are called symmetric because they use the same shared key for encryption and decryption. A standard asymmetric key does not support an encryption context. I used symmetric encryption for these keys so that’s what I am going through.\n\nFinally, we can decode the ciphertext into plaintext using the decode() method. After all that we have access to our decrypted key using the Decrypted variable. As mentioned in the pictured code it’s a good idea to put all this somewhere at the beginning outside the function handler. That way we can have universal access to the decrypted key.\n\nQuickNote\nYou might notice that we grab some additional variables using the os.environ() function. These are built-in in runtime environment variables which we can use to access some metadata from within the Lambda function. Check them out here.\n\nYay, now we can do stuff with our environment variables like pass them along to API calls. Congrats. Celebrate somehow.\n\n\n\nConclusion \n\nSo now you know how to protect ya neck and encrypt ya stuff. Now encryption is a big topic. This is just one small sliver that might be relevant to using AWS Lambda. There is a long history related to cryptography. There is this great book called The Code Book by Simon Singh which delves deep into the history and most ancient implementations of cryptography. I highly recommend it for the power nerds among us who like reading about cryptography.\n\nCreating policies that include encrypting your environment variables means better security and peace of mind. Lamba is a powerful engine and I am enjoying using it so far. My Twittterbot has been running for weeks and it’s cringy as hell so I think I’m doing something right. Now I have the tools to encrypt my API keys and secrets.\n\nAdditional Notes \n\nOne other service that you can also check out is the CloudTrail. You can use CloudTrail and attach them to CMKs so that they can be audited. Cloud trail can keep logs of access which can be used in audits. Very useful for clients with a lot of assets to manage.\n\nOne other additional note is for those who use the AWS CLI there are some commands which might come in handy to know. If you are also studying for an AWS Developer certification you will also need to know these.\n\nCommands:\n\naws kms create-key - creates a unique customer-managed CMK in your AWS\naws kms encrypt - encrypts plaintext into ciphertext by using a CMK\naws kms decrypt - decrypts ciphertext that was encrypted by a KMS customer master key\naws kms re-encrypt - decrypts ciphertext and then re-encrypts it with KMS\naws kms enable-key-rotation - enables automatic rotation of the key material for the specified symmetric CMK. This cannot be done on a CML made by another account\n\n",
      tags: ["KMS","AWS","Lambda"],
      id: 6
    });
    

    index.add({
      title: "Working with Context in Go",
      category: ["Blog"],
      content: "\n    \n\n\nTable Of Contents\n\n  Introduction\n  Context Interface\n  Context in context\n  Context.Background\n  Context.TODO\n  Context.WithCancel\n  Context.WithDeadline\n  Context.WithTimeout\n  Context.WithValue\n  Conclusion\n\n\nIntroduction \n\nWhen you’re having a breakdown caused by the combination of burnout and existential pain, do you get annoyed that your harried cries into the void go unanswered? Well, I can’t help with that, but I can suggest some methods for timing out calls to external or internal services. I’ve been doing research and playing with some of the standard libraries in Go and one of them I find most useful is the context library. Used to get some control over a system that might be running slowly for whatever reason or to enforce a certain level of quality for service calls this small library is a standard for a reason. For any production level systems to keep good flow control the context library is going to be necessary.\n\n\n\nCreated by Sameer Ajmani and introduced in 2014, the context library become a standard library with Go 1.7. If you have looked through some Go library source code you can find tons of examples requiring a context to be passed along. This is just one I’ve used recently. A context is a deadline you can pass into a running process in your code. This deadline can indicate to a process to stop running and return after a condition is met. This becomes useful when reaching out to external APIs, databases as shown above, or system commands.\n\nThe following supposes that the reader knows about goroutines and channels and how they work together. I am going to deep dive into concurrency after writing about context as the context library is part of concurrency. For now, though, goroutines are lightweight threads that can be started for processes and channels are the pipelines used to pass data between these new processes.\n\nContext Interface \n\nThe context library defines a new interface called Context. The Context interface has some interesting fields laid out below:\n\n\n\nThe Deadline field returns the expected time the work is finished and indicates when the context should be canceled.\n\nThe Done field is a channel that is closed when work done for the context should be canceled. This operation can happen asynchronously. The channel can return as nil if the associated context can never be canceled. Different context types will arrange for work to be canceled depending on the circumstances, which we will get into.\n\nErr will return nil until Done is closed. After which Err will either return Canceled if the context was canceled or DealineExceeded if the context’s deadline has passed.\n\nThe Value field is a key-value interface which will return a value associated with the context as a key or nil if there was no value associated. Values should be used carefully as they are not for passing parameters into a function but for request-scoped data transits processes and API boundaries.\n\nContext in context \n\nWhen creating a context in Go it is easy to write out a static context to store and reuse. So far as I can tell from my research this is not the optimal way to work with the context library. Context should take the form needed for each use. It should be shapeless, or in the words of Bruce Lee be like water. Your context should flow through your code and evolve depending on the need.\n\nThere are some exceptions to this. For higher-level processes, you can pass in an empty context when you do not yet have a context in which to pass. These can work as placeholders before being refactored.\n\nContext.Background \n\nThe “Background” function returns an empty non-nil context. There is no associated deadline and no cancelation to speak of. This can be typically used in the main function, for testing, or for creating a top-level context to be made into something else. Looking into the source code you can see that it doesn’t have any logic other than returning an empty context:\n\n\n\nQuickNote:\nTypically, the context is named ctx when it is declared. I’ve seen this in most implementations of context so if you come across ctx in random spots in source code there’s a good chance that it is referring to a context.\n\nContext.TODO \n\nThe TODO function does the same thing. It returns an empty non-nil context. This again is a use case for higher-level functions that may not yet have a function available to use them. In many cases, this would be used as a placeholder when extending your program to use the context library. If you checked out the talk by Sameer Ajmani about the introduction of the context library while refactoring their code at Google they would use the context.TODO to start introducing context into the Google code base without breaking anything.\n\nQuickNote:\nOne thing I will also mention is that somewhere along the way it was suggested that the TODO would be compatible for use in static analysis tools for seeing context propagation across a program. This from what I can tell might have been an off-hand comment from the person who wrote out the notes in the source code. I’ve been looking for the last couple of days and from what I can tell no such tool yet exists. I would investigate how to create such a tool but I’m going to go watch a movie instead.\n\n\n\nContext.WithCancel \n\nLet’s say I’m building a website to review movies. There is a myriad of APIs designed for serving movie information. One of the recent ones I’ve come across is the Studio Ghibli API which is a public API we can just grab stuff from. So, for the special section of the website for Studio Ghibli movies, we’ll use this. The WithCancel function returns a copy of the parent context passed into it with a new Done channel. The new Done channel is closed either when the cancel function is called or when the parent context’s Done channel is closed. Whichever event happens first.\n\nBelow is an example in action:\n\n\n\nHere we are going to simulate a process that is hanging up using the longRunningProcess function. In this example, the function is screwing up but we must run it before we request the JSON data from the API. The “longRunningProcess* function will return an error that will cause the cancel() function within the context to fire.\n\nFor the ghibliReq function we will set up a simple HTTP request using the API and pass a string for locating stuff from the API. Once we set up the request, we have a case statement which will receive channel data. Depending on what happens first the select statement will be sent either the current time or the “Done” channel from the passed in context. If the Done channel is closed we error out, if not we will return the status code from our request.\n\nOur main code starts with setting up the context with a new Background() context which is then passed into a WithCancel() context. The new ctx was passed in an empty context so nothing has happened yet. We then create a new goroutine to create a new thread and call our longRunningProcess. Once that is called we check for errors, which will return since we engineered it that way, and if there are errors we can call the cancel() function in our context. Finally, we use our context to call our request. After we run this we find that the request errored out since it took too long and the cancel() function was called.\n\nIn this example, we are running our longRunningProcess before our request because that is needed before we call our request. If the function errors out we need to be able to call “cancel()” so that we can error out the ghibliReq() function. The way we set it up we are calling cancel for our context before the function has a chance to run. This is intentional to show how the cancel works. We could easily change the time.Sleep() in longRunningProcess to say 1000 milliseconds and our request function will run before cancel() is called but in a production environment if the goal is to make sure we maintain the flow of the call stack we would make sure we’re not returning errors and not calling cancel() for this context.\n\nQuickNote:\nKeep in mind that a context-specific call shouldn’t be a blocking action unless necessary. It’s all about keeping stuff running.\n\nContext.WithDeadline \n\nThe WithDeadline function requires two arguments. One is the parent context and the other is a new time object. The function will take the parent context and adjust it to meet the new time object which was passed in. There are a couple of caveats. If you pass in a context that is already earlier than the passed in the time object then the source code will pass just return a WithCancel context with the same cancellation requirements as the parent which you can see in the source. The Done channel is closed after the new deadline expires. You can also manually return the cancel function or it will close when the parent context’s Done channel is closed. Whichever of those events happens first.\n\nBelow we can go through how the WithDeadline works:\n\n\n\nWe’re going to continue with the idea that we are putting together a movie review site. To be honest it would not be far off character of me to start a website dedicated to talking exclusively about Studio Ghibli movies. The example above is doing something like the withCancel example. We are going to reuse a function to demonstrate our context. Reuse the stuff that works, save yourself some time. We are going to make a request and return the status of said request. The difference is how we handle our context.\n\nHypothetically, we need to create a whole bunch of these cascading requests and we want to make sure that everything is happening on time throughout the call stack. To keep track of time and gracefully error out when needed we can continue to use the deadlines and augment the time for the additional calls. In our example, we create a Background context, then pass that in along with a new time. Now we get a returned context in our ctx variable for about 1 second. In our example, if the request process takes longer than 1 second our context calls the cancel function and closes the Done channel causing the request to error out.\n\nWe can see that this is dependent on the standards that we set. Setting a time implies that you have a decent idea about how long something should take. Which can be dependent on your server availability, internet connection, hardware constraints, etc. I have also seen people grumble about certain service level agreements guaranteeing the return of assets within a certain time frame. With the aim of usability in mind using context, deadlines can help to ensure that we can pull information at a reasonable amount of time and return if not.\n\nContext.WithTimeout \n\nThe next relevant function is the WithTimeout function. This is a slight variation from the WithDeadline function. With a need to make something original in mind the WithTimeout simply returns a WithDeadline context with the time argument passed in added to the deadline. In other words, it acts similar to the WithDeadline in that it will take the parent and augment the time to return a derived context with the new time added to the time before the cancel function is called and the Done channel is closed. I’ll make this example even simpler:\n\n\n\nSame as the example before we set the timeout to close the “Done” channel after the allotted time. In our case, if after a half-second, we’re still waiting for the call we timeout. I love the HTTP go library because it has a built-in function for returning a shadow copy of the request with the new context added.\n\nContext.WithValue \n\nThe last bit of the source I am going to touch on is the ContextWithValue function. This one is a bit controversial since the nature of it, from what I can tell, goes against what the context should be. A context should be a way to ensure that we keep data flowing to and from our programs. The value part of the context though can be used to carry information back and forth. The function allows you to pass in a key-value interface to pass around with your calls.\n\nFrom the original post about context “WithValue provides a way to associate request-scoped values with a context”. I’m going to talk a little about what it shouldn’t be used for. Most articles or tutorials I came across seem to agree that passing information that lives outside of the request itself was a bad idea. DB connections, function arguments, anything that is not created and destroyed within that request is probably not a great design pattern. That said passing values along your context can be useful.\n\nLet’s check out some code:\n\n\n\nWe’re going to use the same code from the last example. Only in this case, we are going to create a new function which will calculate a fake request ID. Say I want to keep a database of all my requests, because… I don’t know, I’m a psychopath. Or I work for the NSA and I’m making some spyware to look in on my ex in the name of national security. And because they don’t train me in operational intelligence, I don’t know how to discern the data that indicates something and white noise, so I collect everything. Even innocuous calls to an open API for looking up animated movie information. I’m very tired right now.\n\nIn our example we do the same as above; set up a context with a timeout for half a second. Only now we have a helper method that will calculate a new request ID and we will use the context to pass that ID along within the context as a new interface that we can access and do stuff with. In this fake scenario, we would log this and close out the context. This will conform to our self-imposed standard of keeping only information relevant to that call. Yay information!\n\nThere is a lot more to be explored about passing along values within a context. I have seen articles where middleware is used to do stuff in between two services to make something work better. I might dig deeper into this and since it’s a bit outside the scope of this I might write about it later. Who knows, I need sleep.\n\nConclusion \n\nThe context library helps to add some sanity to calls in our program. When designing a program incorporating a context in our functions should happen as early as possible. As mentioned, before it is easy to create our function with a TODO as a placeholder and go back when refactoring. It was also mentioned that programs should be created to fail gracefully as well. Take it from someone who spent a long time creating vague fail messages which no one can understand including me. A user should not have to know that a call to something failed just that they aren’t getting their movie title in half a second.\n\nA cool way to picture how useful these contexts can be was touched on in Sameer’s talk. He spoke about the practice of hedged calls where you call out redundant services and take the one which takes less time. It’s all about speed and optimization with them Google people. That is one way in which creating a context to flow through your program would be helpful. When one comes back you cancel out the other which releases the resources that thread might have been using up. The context is a small but very powerful library, it should be used often and with plenty of thought and planning into how it should flow into your program. My hope after reading this is that we all come away with a better understanding of context and how we can use it! If you liked this, had questions and or comments, or you just want to berate me on how much the Last Jedi sucked (it was an imperfect but powerful movie for a world not yet ready for it) hit me up on Twitter! I love topical references.\n",
      tags: ["concurrency","Go"],
      id: 7
    });
    

    index.add({
      title: "Creating a Twitter Bot Using AWS Lambda and Go",
      category: ["Blog"],
      content: "\n    \n\n\nTable Of Contents\n\n  Introduction\n  Twitter App Set-Up\n  Twitter Bot Code\n  Setting up a Lambda Function\n  Conclusion\n\n\nIntroduction \n\nMost people have heard of AWS and developers have started learning how they can use it to further augment the quality of their projects. Recently I have begun the process of becoming one of those people. So far it has been an enlightening deep dive into the different services they offer. It’s hard to get your bearings with something as huge as AWS so for my learning journey I decided to focus on projects I thought would be cool and see how AWS might help facilitate what I build.\n\n\n\nOne project I have been wanting to get into is a Twitter bot using a new language I have been learning, Go. A simple twitter bot is easy to set up. Their API makes it easy to interface with and there are thousands of libraries that utilize this API. I am all for anything which makes my job easier. Let’s see how much easier still I can make this using AWS Lambda.\n\nLambda is a compute service for running code on the cloud. With which you can create functions and triggers to start those functions and Lambda runs your code without having to provision a server. It’s serverless. I realized this is uniquely appropriate for things like a regularly scheduled Twitter bot. With Lambda I do not need to spin up an EC2 or a local VM instance and continually run my bot. I can simply set a schedule and create the bot and Lambda starts up, runs my function, and powers down.\n\nLambda’s free services are also a great use case because the first 1 million requests per month are free. After which you pay $0.20 per additional million requests; The first 400,000 GB seconds are also free and $0.0000166667 for every GB second after that. This Twitter bot is a quote regurgitation service so there is very little chance of it ever exceeding any of that.\n\nTwitter App Set-Up \n\nThe Twitter API has been in v1.1 since 2016. In 2020 however, they began rolling out v2. Rebuilt from the ground up they also overhauled their pricing. For our purposes, we can still use the free tier. Let’s talk about the Twitter end first. If you go to Twitter’s Developer portal you can head to the dashboard and set up a new app. We are assuming here that you have already set up a developer account with Twitter. If you have not, check this out on how to get started.\n\nOnce we get the developer account set up we can head to the Apps portal. Here we can click the top right to create a new app.\n\n\n\nThere is some information required for setting up an app. You need to provide information about the app and a website if the app is going to be attached to one. Once that is set up, we can grab our keys and tokens so and start accessing Twitter through the API library.\n\n\n\nQuick note:\nMake sure to check the permissions for your Twitter App. By default, the permissions are set to read. So, if by the end you see permission errors from the API this might be the culprit. Make sure to set the permissions to the least necessary. Here we are making a bot to post Tweets, so we need “Read and write” permissions. I do not need the ability to DM anyone, so I chose “Read and write”. So, go to the “Keys and tokens” section, copy your consumer key, consumer secret, access token, and access secret. Now that we got our credentials, we are ready to party\n\nTwitter Bot Code \n\nCreating a custom library for dealing with the API is preferable for any production system as it allows for the customization needed. For our purposes, we only need to open a stream and send a tweet through it to post. So, I decided to use the Anaconda Library which still uses v1.1 of the Twitter API.\n\nThe first thing we do is set up our development environment. I use VS Code with the Go plugin to help me. I also use a Windows environment to make everything. Later I will circle back to why this isn’t a great idea. We are also going to use Go modules to keep track of our dependencies. Go 1.11 and up can support Go modules and anything above Go 1.13 is going to use them by default. With that in mind, we set up our file directory as such\n\n\n  Bot\n  go.mod\n\n\nWe CD to the root of our project and run:\n\ngo mod init bot/main\n\n\nNow we have our directory and our mod file which is where our dependencies are housed. Next, we create our bot file.\n\n\n  Bot\n  go.mod\n  bot.go\n  quotes.json\n\n\nYou will also notice that there is a JSON file called quotes.json this is where we will store our quotes for now. Now we can get started building. The first thing is we add our dependencies. I added a few here and I will explain the need for them as we go along.\n\n\n\nThe next part you will see is our data types we created for the bot.\n\n\n\nHere we created structs to house some data. Structs in Go are a way to group related data. I have found them to be some of the most useful data type collections to perform a variety of tasks with. The first named APICred is just a way to house and reuse the API credentials that we grabbed from the Twitter App we created.\n\nThe next structs are for the quote object we will be grabbing from the JSON file. We have a struct made for the individual quotes and one for an array of the aforementioned QuoteObject structs. This makes it easier to grab all our quotes and use the list to set our bot logic. This is not an ideal solution as it assumes we will always grab all the quotes from the list. This is not a scalable solution as we would not want to grab all the entries from say a database and then sort through them manually. However, for this low threshold example, it works.\n\n\n\nQuick Note\nYou will notice for the struct keys that they are all capitalized. This is good practice because for me it looks neater and some issues can come up when working with things like JSON data. When unmarshalling the data into struct fields those fields are only exported if they are capitalized. This allows the JSON package to use the field names to unmarshal the data from the JSON file for use in our bot. I learned this after seeing my data come up blank when grabbing the quotes from the file. You can also see the issue in this Stack Overflow article and save yourself some time.\n\nAnother thing to note on the structs is the tags or annotations attached to the struct keys. In many cases, this might not be necessary. As in the case where everything is named the same on the JSON fields and the struct fields; The package should be smart enough to match them. However, I program as defensively as I can so the JSON tags allow for me to match them up directly. The annotations tell the JSON package that the TweetId field needs to be matched with the tweetid field in the JSON file. The tags do that for all of them. The QuoteList field will grab that entire “quotes” block from the JSON file. This also works when working with binary JSON or BSON information, say from grabbing information from Mongo DB.\n\nHere is the next block of code for facilitating some functions in our bot:\n\n\n\nNow comes some more fun. When working with local files stuff like tokens, keys, DB params, etc we would use a .env file and import it into our code. In this case, we are not using this locally or on a production machine, we are creating an AWS Lambda function! The env variables can be set on the Lambda dashboard. On the code side, we do not need to load an environment library, we can just use the os library to load our environment variables directly from AWS. Above we create a new instance of the APICred and call it env and return that from the function. Here we make use of Go’s return type naming in the function signature to initialize our returned item. Then we set the fields and we are ready to party rock the Twitterverse.\n\nThe next function is a helper function to come up with a random index used to randomly search for a quote to tweet out. We use the math/rand library along with the Seed function to get a random index within the range of quotes that we have. We serve a minimum index value which should always be 0 (maybe make a const next time?) and a max value. In this case, we will count the number of entries (quotes) and use that as the max value. We do this so that we can always come to an index with the range of quotes we have. In other words, if we have ten quotes we want to return an index between 0-10 and use that to match to the tweetid field in the JSON file and grab a quote. The “Seed” function here is important as the rand function is deterministic. As in it will return the same value each time you run it. The “rand” function is considered pseudo-random for this reason. If Seed is not called rand will by default using Seed(1) which will get us the same number each time. By using the Seed function to set the seed on each run and using the Unix time will make sure to feed in an int64 number representing the time that changes every second.\n\nNext, we come up with the logic to grab our quote:\n\n\n\nThis function is used to get a quote and return it as a string to our API object, which then sends the string to Twitter. The first part is to grab the file and return a file object using the os.Open function. This assumed the JSON file is in the same directory as the bot file. If not an error is thrown. We set up the defer quote_file.Close() so that all the other logic in our function runs and the defer runs last. This keeps our file open for us until we are done. This is great for flow control.\n\nNext, we need to convert what is in the files into a []byte list so that we can unmarshal the data. The ioutil.ReadAll function call allows us to assign the contents of the file to a byte object. After that, we unmarshal the data using the json package. Here we set it to unmarshal the quotes_bytes object into a pointer for our quotes object which is initialized in the line above that. Here is where our annotations go to work and make everything work automagically. We set some error handling and then we get to work getting our quote.\n\nWe set the maximum number of quote objects by referencing the QuoteList field and counting the number of objects in the array. Next, we get our random_tweetid which will grab a random index from zero to the max number of quotes. Finally, we get our quote by calling the Quote field from one of the random indexes in our QuoteList which returns a quote ready to be served to Twitter.\n\nThe last part of our bot puts everything together and execute:\n\n\n\nWe load our environment variables using the APICred struct and using the Anaconda library we set our Consumer key and secret and then create the API instance using the access token and secret. Now with all the power at our fingertips, we call our GrabQuote function which grabs a random quote from our JSON file and sets it to the tweet variable. In the PostTweet function call you can see that we pass in our quote via the tweet object and we also pass in url.Value{} which we can call from the net/url library import. If you look through the Anaconda source, in the PostTweet function the url values are used to set some required API fields. Once those are set the library creates a channel to pass in all the fields to send to the API. Diving into how the libraries source is a good idea. You should know how every line in your program works. This includes knowing how a library handles inputs and outputs. We set some error handling, just in case, for the PostTweet and we send the tweet through the API and into the timeline.\n\nEvery Go program has the main function where everything is called. Here we utilize the AWS Lambda for Go library to call our function. The lambda.Start() is how Lambda will call our function. In our case, we house all the logic in the SendTweet function and use the logic in main() to call it. If we were creating say an API we would create a handler function which is called from the lambda.Start() function. In our case I decided not to return anything however, Lambda allows for the return of between 0-2 arguments. This is good for returning logs of the events run and or errors. AWS Lambda offers great tools for logging the actions of your function and the returns are how they are recorded. If setting returns one of the returns must implement an error according to the docs. So you can say return a success log with some information you want to track and an error for when stuff goes wrong. AWS Lambda also allows for passing in between 0-2 function arguments. They do say in the documentation that if passing in an argument a context is required. I have not tested this yet, but I will, and I will probably write about it.\n\nOur bot is done! Let’s party.\n\n\n\nBut Wait, we can’t party yet!\n\n\n\nBefore that, we need to set up a way for our bot to run. It would suck to have to manually run this every time I want to send a quote. Might as well open an alt account and do it manually. It would suck even more to have to provision a cloud instance to run all day for the sake of posting a tweet once or twice a day. Screw that, let’s use Lambda.\n\nSetting up a Lambda Function \n\nHere we are going to assume that we already have an AWS account set up. If you don’t go set it up. It’s useful to have. Now that we’re set up go find your AWS Management Console and search the services for Lambda:\n\n\n\nClick Lambda and you will be brought to the Lambda screen. Here you can find a list for all your applications, functions, and layers. In addition to all that you have a dashboard dedicated to monitoring all your Lambda stuff.\n\n\n\nIt will show the number of functions you have going, the storage used up by your functions, your concurrency, and tons of other stuff. We mentioned the pricing for Lambda; The prices vary depending on the amount you allocate. The pricing also takes into account the duration of time it takes for your functions to execute as well as the number of requests. For us, we are using far less than what is allowed on the free tier since we are only invoking once a day at this point. This will change as I further develop the project.\n\nNow, let’s make a function:\n\n\n\nOn the next screen, we have some information to fill out. The first is authoring options. Here we can leave the default “Author from scratch” option selected. There is also the option for using blueprints to develop our app. If you select this, you are given a list of different already made sample projects which you can use to quickly develop apps. You can also filter out what you are looking for. This makes it easier to quickly prototype an app and start standing up your back end. There is also another option to Browse serverless app repository where you have a similar solution for quickly standing up apps. For our purposes, we are going to author from scratch.\n\nWe put in our function name and select the runtime. The runtime we will circle back to but be sure to select Go 1.x which will set us up to use Golang as our function language. For the permissions, we default to Create a new role with basic Lambda permissions which is fine for now. If you have existing roles created in your IAM console you can feel free to use one. Just be sure that the role has the correct permissions for running Lambda functions. You can also Create a new role from AWS policy templates which allows you to create a role using policy templates built by AWS.\n\nNow we are ready to stand up our function. We are taken to the home page for the function. Here we can start setting up everything:\n\n\n\nThe first thing we are going to do is set our environment variables. As I mentioned above, we are not loading in an env library and we are getting our variables directly from AWS. Here is where we set those up:\n\n\n\nIf you scroll down you will come to a section labeled Environment variables, hit the Edit button and you are taken to the next page where we can set up our vars. If you hit the Add environment variable you can start entering in all your needed variables. In our case, we set up one for the consumer key, consumer secret, access key, and access secret. As noted in our code we will import them in our code using the os.GetEnv function. AWS even notes in the margins how this is done for each language. So spice. AWS lets us set up encryption for our variables as well. This will allow for sensitive information (like API credentials) to be encrypted on the Lambda console and when being used by the API. AWS provides a Key Management API service for encrypting and decrypting your sensitive information. I will not go into how to use the API here but setting up encryption is probably a good idea, so I will write a follow up about it at some point. Once all our variables are set, we can go back to the main page and upload our code.\n\nThe function code is usually available for editing in the “Function Code” section. There a Cloud9 IDE is set up for looking at and editing our code. I have used it and it is a useful tool. I don’t think it is compatible with the Golang runtime though. I will come back to this as I ran into issues when building my function.\n\nNow we have to bundle our code and put it into AWS through our newly made function. As mentioned above I used Windows to create this package so I will go over what I did as reflected in the docs. However, I ran into issues when using Windows to do this. Based on my experimenting and some feedback from some smart people I found that I needed to use a Unix based endpoint to build out the project. I will break down why after.\n\nOnce we have everything set to go on the AWS side, we are ready to build our package. So CMD your way to the root of the package. Now we will set our build environment to Linux using this command:\n\nSet GOOS=linux\n\nWe do this to set the build command to tell the compiler to use the Linux binary. So far as I can tell this is a bit of a known issue with using Windows to do all this. This has not been solved as of the publication of this, but I am sure a fix will come through. Earlier when we set our runtime to Go 1.x we were also setting the server operating system which runs our function. When we say “serverless” we mean someone else’s servers. In this case when setting the Go run time we are setting the OS to run the Amazon Linux operating system. Amazon Linux is a server operating system made to run AWS based services. You can also create images using Amazon Linux for running tons of stuff.\n\nAfter we set the binary, we can build our package:\n\ngo build -o main bot.go\n\n\nIt is important to make sure our executable is called main. As the main function is the invocation point and we are going to set the handler to run main. We need to upload the executable created by the build command to our Lambda function as a zip file. So we send our main executable to a zip file. In our case, we need to zip up the main executable and the quotes.json file since we will need that for our quotes. I did this manually but there is a command you can run for zipping up everything. Shown below:\n\n%USERPROFILE%\\Go\\bin\\build-lambda-zip.exe -output main.zip main\n\n\nQuick Note:\nSo far as I can tell when you zip the contents of the package using Windows it retains Windows permissions. Retention of POSIX permissions is a known issue with using Lambda cross-platform. POSIX permissions are a Unix standard which defines the permission structure used to interact with files and applications. From what I can tell these permissions are not set correctly when doing all this in Windows. However, when I used my Mac (a Unix based OS) I was able to build and test my function without issue.\n\nAfter we zip up our executable and our quotes file we can navigate down to the “Function code” and upload our zip:\n\n\n\nSelect the Upload as zip option and we can upload our newly created zip file.\n\nThe final step is we need to scroll down to the Basic Settings section and select Edit. The edit basic settings page is where we can set a description, change our run time, edit the handler, set the memory, set a time out period, and edit the Execution rule. One thing to note is the memory that you can set to whatever you would like. The default 512 MB is fine, especially in our case. But keep in mind that the more memory the more it may cost. Additionally, you may not need all the memory you set and that could end up costing you money needlessly. For our purposes, it’s fine where it is.\n\n\n\nWe need to edit the handler and set that to main. As I mentioned above, we are using the main handler to invoke our executable within Lambda. So, make sure that when you build your package you name it as main so that the handler will work.\n\nCongrats! Now that we are set up, but wait! We should test this. On the top right of the function home page, there is a drop-down for selecting a test to run. Clicking this gives us the option for setting up a new test. You can use this to set up regular scenarios, edge cases, or any other specific tests you want to run. For us we just need to hit “Configure test events” and this will bring up the test creation page:\n\n\n\nHere we will want to select the Create new test event which will allow us to select an event template. The default Hello World template is fine for now but there are built-in test templates which you can choose from depending on your needs. Once this test is created it can be used as a test template for our function. You can leave the default JSON there, and make sure to name your test event. The JSON can be edited to accommodate whatever you are using. For example, we can test the ability to run a Lambda function via an Alexa trigger. So, the testing JSON file would be where you would pass in echo API session information. For now, the default is fine, when we’re done hit Create. Now we can test our function by hitting the Test button:\n\n\n\nHuzzah! We tested good. Now what? Well, I think we would like to run this bot on a schedule. Lambda allows for a lot of different triggers but for our purposes, we want to run on a regular schedule. Like a cron job. To do this we select the Add trigger button on the Designer section of our function home.\n\nThen we are taken to the trigger page where we want to search for EventBridge (formally CloudWatch Events).\n\n\n\nNext, we hit select the rule for EventBridge:\n\n\n\nWe can reuse old rules we have created or we can hit the Create a new rule option. That opens a bunch of configuration options. We can set the name of the rule, whether it is scheduled or based on an event, and if we want to enable the trigger. We want to schedule this to run daily so we select the Schedule expression option and then enter cron(0 12 * * ? *) as our expression. This is an awesome reference for the cron syntax. This is set to run daily at noon Keep in mind for cron expressions UTC is used. This will run every day at noon UTC. Then we hit save. Now we are done! We have officially added to the background noise of Twitter!\n\nConclusion \n\nDoing this allowed me to check out how cool and simple running stuff off Lambda is. I most likely will continue to do stuff with it as I continue to develop this project. A couple of things I will experiment with is using a Cloud9 IDE available in AWS to develop things like this. It is Unix based so performing the build functions won’t pose too many new issues. It looks as if AWS is bringing the users to the cloud, even by force if needed. It’s not a bad thing though, nothing is perfect, and this is a great solution for running serverless back end services.\n\nThe second part of this series will focus on integrating a database into our Lambda function. Putting six quotes in a JSON file might be an interesting challenge to pull from but it hardly makes for a good way to send out information or useless quotes in our case. I am also going to explore possible language interpretation to create a trained bot that throws out randomly generated quotes based on input data. I’m not looking to replicate the racism, sexism, and pure unadulterated horribleness flying through the Twitter pipes. So, it will be an interesting challenge to try and filter much of that out. We shall see. For now, use Lambda, it’s awesome!\n\nNow we can actually party!\n\n\nFull Github Code\nAWS Docs\n",
      tags: ["AWS","Go"],
      id: 8
    });
    


var store = [{
    "title": "Dockerizing System Tests With Selenium",
    "link": "/blog/dockerizing-system-tests-with-selenium.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2022-03-10-dockerizing-system-tests-with-selenium/selenium.png",
    "date": "March 10, 2022",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Selenium WebDriver Setup Running Tests Notes Conclusion Introduction We are spinning up a new application for..."
},{
    "title": "Shared Memory in Docker",
    "link": "/blog/shared-memory-in-docker.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2022-03-05-shared-memory-in-docker/cover.png",
    "date": "March 5, 2022",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Shared Memory Device Use in Docker Conclusion Introduction Recently, I was experimenting with system testing on..."
},{
    "title": "Using Amazon API Gateway with Lambda",
    "link": "/blog/using-api-gateway-with-lambda.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2022-02-09-using-api-gateway-with-lambda/cover.png",
    "date": "February 9, 2022",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Amazon API Gateway Setting up an API Adding the Trigger Setting the Methods Conclusion Introductions I..."
},{
    "title": "Shenanigans with Shaders",
    "link": "/blog/shinanigans-with-shaders-copy.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2021-11-21-shenanigans-in-shaders/cover.jpg",
    "date": "November 21, 2021",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Shaders Setup Shader Code Conclusion For those of you who love rabbit holes, learning graphics programming..."
},{
    "title": "Messaging and Madness: Sending Messages with AMQP and Amazon MQ",
    "link": "/blog/messaging-and-madness-sending-messages-with-amqp-and-amazon-mq-copy.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2021-10-30-messaging-and-madness/title_card.png",
    "date": "October 30, 2021",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction AMQP AMQP and Amazon MQ Serialization Conclusion Introduction How do software systems talk to each other?..."
},{
    "title": "Health Checking S3 and DynamoDB in AWS",
    "link": "/blog/checking-health-in-s3-and-dynamodb.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2021-03-03-checking-health-in-s3-and-dynamodb/cover.jpg",
    "date": "March 3, 2021",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Problem S3 Solution DynamoDB Solution Conclusion Introduction A hybrid infrastructure has tons of exciting challenges. Although..."
},{
    "title": "Encrypting Your Environment Variables in Lambda with KMS",
    "link": "/blog/encrypting-your-envioronment-variables-in-lambda-with-kms.html",
    "image": "https://georgeoffley.com/assets/images/encrypt-lambda-envs.jpg",
    "date": "September 4, 2020",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Key Management System in AWS Customer Master Keys KMS and Lambda Conclusion Additional Notes Introduction Do..."
},{
    "title": "Working with Context in Go",
    "link": "/blog/working-with-context-in-go.html",
    "image": "https://georgeoffley.com/assets/images/working-with-context-in-go.jpg",
    "date": "August 17, 2020",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Context Interface Context in context Context.Background Context.TODO Context.WithCancel Context.WithDeadline Context.WithTimeout Context.WithValue Conclusion Introduction When you’re having..."
},{
    "title": "Creating a Twitter Bot Using AWS Lambda and Go",
    "link": "/blog/creating-a-twitter-bot-using-aws-lambda-and-go.html",
    "image": "https://georgeoffley.com/assets/images/twitter-bot-aws-lambda.jpg",
    "date": "August 8, 2020",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Twitter App Set-Up Twitter Bot Code Setting up a Lambda Function Conclusion Introduction Most people have..."
}]

$(document).ready(function() {
    $('#search-input').on('keyup', function () {
        var resultdiv = $('#results-container');
        if (!resultdiv.is(':visible'))
            resultdiv.show();
        var query = $(this).val();
        var result = index.search(query);
        resultdiv.empty();
        $('.show-results-count').text(result.length + ' Results');
        for (var item in result) {
            var ref = result[item].ref;
            var searchitem = '<li><a href="'+ hostname + store[ref].link+'">'+store[ref].title+'</a></li>';
            resultdiv.append(searchitem);
        }
    });
});