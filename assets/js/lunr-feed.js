var hostname = "https://georgeoffley.com";
var index = lunr(function () {
    this.field('title')
    this.field('content', {boost: 10})
    this.field('category')
    this.field('tags')
    this.ref('id')
});



    index.add({
      title: "Browser Testing With Cypress",
      category: ["Blog"],
      content: "Introduction\n\nWhen I write code, I try to remember that everything is broken until proven otherwise. I’ve been shifting my learning to browser-based platforms that I am working on and with that shift, browser tests have started to be folded into that mindset.\n\n\n\nCypress is a tool for browser tests that I’ve started learning, and it’s been an interesting week. I’ve written about browser testing before, so this will be a bit less comprehensive and will mostly be my thoughts on the tool and how useful I think it is. Complete tutorials for creating end-to-end browser testing solutions using Cypress can be found in their docs.\n\nBrowser Tests\n\nTesting has a lot of different connotations. In this context, browser tests are tests that try to use your app and tell you if something is broken. Cypress is one such tool, but there are many others.\n\nCypress Setup\n\nCompared to something like Selenium, the setup was simple. Cypress advertises as an all-in-one solution for end-to-end testing. And so far, that’s been true. Selenium requires a web driver and external resources to function, whereas Cypress seems to work quickly out of the box.\n\nTheir docs suggest setting up the cypress commands in your package.json file for easier running. This is a good approach to test your app how you need to.\n\n\n\nTo help with your setup, you have access to their testing GUI.\n\n\n\nThis is a vast improvement over other tools I’ve learned since everything can be set up in a GUI, including choosing a browser to do your testing on.\n\nI’ve only been able to try end-to-end testing, but I’d like to dive a little more into component testing. More to come.\n\nSimple Cypress Tests\n\nSo the tests themselves are relatively simple. The scripts only need a few lines of code, as most of the setup is in the project config and support files. Another great advantage over other tools I’ve tried so far.\n\nA test might look something like this.\n\n\n\nThe above test, taken from their docs, is put into a describe() block. This gives us flexibility in creating testing classes and organizing our tests. Within that block is another block, the it() block, which is our test itself. We can be as granular as we want with our tests. For example, if we want to write an it() block for every step in an end-to-end test, we can do so with the confidence that we’ll see which stage failed.\n\nThe cy.visit() is a command and assertion. So, in this example, if a website is not sending a response, we can see this test fail. So this gives us the flexibility to write out the exact behavior we expect.\n\nSo we can add all the steps needed to test a part of our app. Like so.\n\n\n\nThis example, also taken from their docs, takes advantage of everything described above. It also introduces chaining, which we can use for multi-step processes.\n\nTesting GUI\n\nOK, on to some of the stuff that makes Cypress a fun tool to dive into. Namely the GUI.\n\nBrowser-based engineering is still a space I’ve only been in for a while, so this might be new to me, but adding a browser tool for developing your apps blows me away. Let me tell you why.\n\nSo we wrote the test above. Great. Now we need directions to tell a Cypress how to navigate our app. So we need some selectors, which are always murky to me as relying on class selectors is a brittle test that can be broken with a new release. We need something better, and it’s not always clear what that “better” may be. So in comes the testing playground.\n\n\n\nThe above, also taken from their docs, demonstrates what made using Cypress enjoyable. Their testing suite gives you valuable tools like a real-time test runner, a test builder, a time machine so you can go through each testing step to see where issues might pop up, and an advanced selector playground.\n\nThis tool does a great job of helping you create robust selectors for your test. It allows you to hover over elements, highlight them, and the selector playground will provide you with their best guess for a cy.get() command so tests can be filled out quickly.\n\nSome More Cool Stuff\n\nJust a couple more things I wanted to mention. One was how easy their API makes it to creating new commands in their API for common functionality to your testing apparatus.\n\nFor example, we created one which goes to our site, types in login info, and clicks the login button. Now we have all that functionality in a single command for use in any test we need authentication for. Before long we’ll have tons of them so we can focus on testing our platform’s behavior.\n\nAnother was using what they call fixtures for injecting static data into your tests, which I love. Static data, like login credentials, names, etc, are much cleaner when you can create a JSON file and utilize the existing API to access the necessary values. I love that; it makes maintenance so much easier.\n\nConclusion\n\nAfter all that, I can confidently say I enjoy writing tests using Cypress. It allows me to write tests the right (easy) way. Testing is often not something people think about. So make it easy to do, and Cypress does that. I’m excited to continue learning.\n\n-George\n",
      tags: ["Cypress","Testing","JavaScript","TypeScript","Newbie"],
      id: 0
    });
    

    index.add({
      title: "Adventures In TypeScript: Typing My Way Out of Problems",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  The Problem\n  Rendering A Link\n  Objects And Typing Issues\n  Cheating\n  Another Solution\n  The Better Solution\n\n\nIntroduction \n\nI’ve been taking more and more tickets with the new gig. This week I was tracking down some text to make some updates, and this was not as simple as I thought it might be, so I decided to take some notes and reflect on some stuff I learned.\n\n\n\nThe Problem \n\nThe task was simple, update some text on the platform. I quickly discovered that it was not as straightforward as I thought.\n\nThe first issue was trying to track down where some of these values which needed changing lived.\n\nThe second problem was the updated content included a link to some external docs for our platform.\n\nRendering A Link \n\nSo I found where the objects storing the text were. That problem is solved. On to the next\n\nSo how do I add a link to these strings I mentioned? It seemed easy; you replace the string, but how do you make the link element work?\n\nThe solution should have been more obvious to me at that point. But sometimes I forget stuff; I’m human. I’m allowed. So let’s go through my process.\n\nI thought I could just put an href tag into the string value I was updating. Like so.\n\n\n\nAnd I pass it into the JSX like this.\n\n\n\nThat won’t work since you end up with this.\n\n\n\nSince JSX will interpolate this as a string or a JavaScript function and won’t interpret pure HTML elements as HTML tags, this isn’t my solution.\n\nSo we needed something closer to this.\n\n\n\nI’m using the prebuilt Link component provided by Material UI, and I needed to get the link passed into this component in our JSX as a string, which solves this problem and takes us to the next problem.\n\nObjects and Typing Issues \n\nNow I have a new goal.\n\nI created an object literal to hold my values and possibly use it to hold URLs for additional help docs in the future.\n\n\n\nThe next step was to pass that URL around. Here’s another layer; I wanted to reuse a string value used to grab all the config options to cut down on hard-coded stuff. So I made the key in my object literal match possible values for this string object. Something like this.\n\n\n\nWe’re using bracket notation to pass in the object key and have it evaluated as the variable key rather than a standard dot notation, which would throw an error.\n\nAlthough you undoubtedly noticed the squiggly line yelling at me, that gave me this message that I didn’t understand.\n\n\n\nWhat I initially did confused TypeScript, and thus the error vomit. If we look at the error, though, it’s less error vomit and more a breadcrumb trail back to where I made a mistake.\n\nOur string variable key is typed as a string, but TypeScript has no idea what the context is since we’re using an object literal without a type. So for TypeScript, key could be anything in my code, and now I’ve confused it. Here’s where I stepped on a landmine.\n\nCheating \n\nMy first solution was to cast my object as the any type.\n\n\n\nThis cleared the error, and I could run the project with the behavior I wanted. Problem solved, correct? Nah.\n\nTypeScript introduced type checking into JavaScript, effectively making it a statically typed language. So the types of objects are known at compile time. This helps create type-safe code and catch problems earlier. The issue with my solution is that it skips the good parts of TypeScript by turning off type checking.\n\nWhy use the language if we’re not going to use the language? I looked for a better solution.\n\nAnother Solution \n\nA not great solution would be to use some magic.\n\n\n\nA bit is going on here. We’re still using the bracket notation, but in the brackets, we’re casting the variable as type string using the as operator; this is also called a type assertion. And then, we’re utilizing the keyof operator in TypeScript to create a union type for our DocMap object, which is passed in using the typeof operator. It’s a solution that confuses me to think about, makes the code harder to read, and introduces a code smell since it seems like a hack rather than a planned solution.\n\nThe Better Solution \n\nA better way might be to use mapped types and interfaces. As a general rule in my TypeScript adventures, I’ve been sticking with making the correct types or interfaces for objects. So following that, I can do something like this.\n\n\n\nThis solution creates an interface for the help doc objects, and then I can create a new object with our data using that type. I tested making a mapped type for the first property as a demonstration. Generally, you use those when making types with properties that might not be known ahead of time. It’s a generic type that we can use to denote the types for the value and keys in any DocMap type object.\n\nMy only issue is that it’s still a bit roundabout for what I want. This is one-time use, and it’s not being exported anywhere. So I want something even simpler.\n\nMy Solution \n\nMy solution was to use a Record type to create an object type with the required property types.\n\n\n\nThe Record type is a utility type that uses the passed in types, via the angle brackets &lt;&gt;, to map the types for the object property names and values. This is type-safe, easy to implement, and checked all the boxes I needed.\n\nConclusion \n\nThis is the strategy for learning the code base. Spend my energy tackling as many problems as I can early. We called this “racking up the cash register” when I was wrestling. The more I take on, the more I learn, and the more I learn, the bigger the challenge I can take on. I hope this helps someone in their learning journey!\n\n-George\n",
      tags: ["TypeScript","JavaScript","React","Newbie"],
      id: 1
    });
    

    index.add({
      title: "Adventures In TypeScript: Destructuring and Code Organization",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  Destructuring\n  Destructuring Imports\n  A Cleaner Way To Destructure Imports\n  My Conclusions\n\n\nIntroduction \n\nI’ve been diving deep into the JavaScript ecosystem via TypeScript. One thing I’m observing in the more established engineering culture I’m part of now is the idea of cleaner code using things like destructuring. I am still new to this type of stack, so I wanted to talk about it and understand how we can better organize code.\n\n\n\nThis is a bait and switch since we’re talking about organizational principles in JavaScript. Both have the word ‘Script’ in the name, so I will stick with it.\n\nJust a warning: I am new to this world. I don’t have a lot of expertise (yet) about what is or not cool in the ESM spec. So some info here might need further study.\n\nDestructuring \n\nDestructuring objects is a simple syntax that yields some helpful behavior. Destructuring allows you to unpack properties and values for arrays, objects, and classes. This is useful in organizing our code because we can avoid things like calling class or object names before their properties.\n\nSo we can go from doing things like this.\n\n\n\nTo something a little cleaner like this.\n\n\n\nWe can also do this in a way to avoid naming collisions for commonly named code that we will discuss further down.\n\nLet’s see what this looks like.\n\nFirst, we set up a new project and made some stuff to export.\n\n\n\n\n\nDestructuring Imports \n\nSo let’s destruct some of these.\n\nSo the first thing I wanted to try was importing and destructuring these imports all in one line. So I was able to get something like this to work.\n\n\n\nSo first, we make constants that match the incoming object names. Then we utilize the require() syntax to pull in our objects. This assigns the two, three, five, and seven names to the constants we pulled in through the require statement.\n\nThis works for the demo project I was messing with, but it’s not the cleanest solution. You undoubtedly noticed the note above and TypeScript yelling at me by underlining words.\n\nRegardless, we have access to the imported values.\n\n\n\nA Cleaner Way to Destructure Imports \n\nThe cleaner way I’ve observed is something closer to this.\n\n\n\nSo these values are imported using the import syntax with a default import and then use a standard destructure syntax where you create constants matching the name of the objects you’re importing. So now we get this.\n\n\n\nNOTE: Default imports are when you have things being exported via the default keyword like this import DefaultObj from ‘ObjMod’, where a named import is when you’re naming the things you’re importing via something like import { ObjName } from ‘ObjMod’.\n\nThis has been the convention I’ve seen most often. I like it; clean, easy to read, and TypeScript isn’t yelling at me.\n\nMy Conclusions \n\nI will do things the cleaner way, but it comes down to what tool works best for that specific situation.\n\nOne thing I would note is including proper naming for object imports. Since we’re talking about taking away the class prefix, it might make sense to do something like this.\n\n\n\nThis combines my preferred way of destructuring along with syntax to rename the imported objects. This practice will help reduce the likelihood of collision bugs if I have several things named like config or something like that.\n\nDestructuring is an excellent practice for clean code. Not like THE clean code, just code I think is well written and easy for the next person to read.\n\n-George\n\nFull Demo Code\n",
      tags: ["TypeScript","JavaScript","Destructuring","Newbie"],
      id: 2
    });
    

    index.add({
      title: "Adventures In TypeScript",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  TypeScript So Far\n  Type Definitions In Functions\n  Generics\n  Union Types\n  Conclusion\n\n\nIntroduction \n\nI started a new job, and the stack is entirely different from what I know. Allow me a moment to break it down.\n\n\n\n\n  First Engineering job stack: PHP, HTML/CSS, VBScript, C#,.Net, and MySQL. No JavaScript\n  Second job stack: Python, Go, MySQL, Flux (custom functional language). Still, no JavaScript.\n  New job: It’s all JavaScript\n\n\n\n\nWell, TypeScript. But even still.\n\nI’ve had to become fluent in a variety of languages, but the JavaScript ecosystem was never on the list until now. Closest I ever got to building anything was writing tiny, mostly stolen from Stack Overflow, APIs using Express. So what am I to do when I get offered a position building some cool stuff with some incredible people? Say no? I don’t think so.\n\nAs you get further into your career, you start seeing new organization’s coding stuff is like everyone else’s coding stuff. It’s just a matter of implementation. So I’ll spend the next few weeks diving in, asking tons of questions, and learning to understand this—no big deal.\n\nIn the meantime, I’ll write about some of the stuff I’ve learned, starting with my adventure’s learning TypeScript.\n\nTypeScript So Far \n\n\n\nTypeScript is nitpicky. If I were to make a list of program languages as cartoon characters, I’d put TypeScript beside Chuckie Finster from Rugrats. Quick to point out the dangers and not quiet about their objections. However, the more I learn it, the more it becomes the thing needed to maintain the chaotic, almost Tommy Pickles-like nature of JavaScript. I’m starting to become a fan.\n\nEveryone needs a real one like Chuck Finster on their adventures.\n\nComing from Python as my primary tool in the last job, I needed to shake off the cobwebs for statically typed languages. Just make sure the types match. No big deal, right? Mostly.\n\nTypeScript takes this a great deal further in its implementations. Let’s go over what I mean.\n\nType Definitions In Functions \n\nIn Python, everything is an object. In TypeScript, objects are just everywhere. One example is the use of objects defined in the function signature.\n\n\n\nIn this example, we define the object options and then give the parameters in the form of function arguments. That makes sense for maintainability. A single type that is only used in one spot is a perfect candidate. It also helps that the IntelliSense will be able to pick up on the type and give you the ability to access those properties in the autocomplete.\n\nGenerics \n\nGenerics are stand-ins used for passing around data types where the type of an object or property might not be known. TypeScript takes these to an interesting extreme. Let’s take a look.\n\nGenerics can be used in interfaces; interfaces are an abstract type used for holding data. See what I mean below.\n\n\n\nHere we define and interface Model and assign it the type &lt;;T&gt;; you use the angle brackets to pass in types. The idea here is that this Model interface can be reused and take the form of many different model types. &lt;T&gt; is just a stand-in and can be used by any type. The generic is then passed into the value property because we don’t know what the value could be.\n\nSo going down into the variable definition for newModel we are looking to pass in a string type, and we do that by using the &lt;&gt; angle brackets. Finally, we assign the value property to the string \"Brenda\". You can see the printout below that. Now we can pass in any data type we’d like in this interface and use that same type as the value property.\n\nCool right? You can do that with functions too! As seen below.\n\n\n\nSo we have a User interface with two properties which will serve as an example of a data type for use to pass in. Then we have a function below where the idea is that it will be reused to grab any kind of resource from an API.\n\nIn the getResource function, you can see where the usefulness comes in; Seeing as we’re not sure what type of data we’re going to get, we can pass in the &lt;T&gt; generic and then use that generic to return an array of types &lt;T&gt;. Here we can safely use the User interface or any other interface. Check out the results.\n\n\n\nWe called the getResource function and passed in the User type, and we can then map out the results from the User type. As seen in the notes, we’ll also have access to the type properties via autocomplete.\n\nUnion Types \n\nSomething that blew my mind was working with union types. It was weird syntax I’d never seen, and it threw me until someone explained it to me.\n\nA union type is a type that can be one of two or more types. Let me show you what I mean.\n\n\n\nWe defined data, which can be either a string or a variable. The pipe operator, |, which is the bitwise OR operator in JavaScript, denotes the different types of which this variable could be. Below are the results from printing this.\n\n\n\nSo we can assign multiple types to this variable through union types.\n\nThis one is pretty simple, but we have union types in the source code at work that could be one of ten different types.\n\nThis is again great for code reuse in that we can use assets differently due to them being able to take the form of more than one type and have different behavior.\n\nConclusion \n\nThere is a ton of other stuff I’ve learned. Much of which is still in my notes and on my Today I Learned repo and in need of digesting.\n\nThis is just the beginning. I have an opportunity to learn so much from this group of engineers, and I am excited. In doing that, maybe I can help someone else learn too!\n\n-George\n",
      tags: ["TypeScript","JavaScript","Newbie"],
      id: 3
    });
    

    index.add({
      title: "Context In React",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  Data In React\n  Context In React\n  Context In Use\n  Conclusion\n\n\nIntroduction \n\nReact offers the ability to build out web applications quickly. One of the details of this strategy includes passing data to different components in the DOM. This can consist of needed data to build features, states of a component, and anything you can think of.\n\n\n\nData In React \n\nOne great pattern in React is how data can be passed through the different components. However, this can get messy.\n\nAn application might be broken down like below.\n\n\n\nA collection of components. Some of these components return other components. These are called parent components, and their children are nested components.\n\nWe can pass data back and forth throughout the lifetime of each component. Working with only props, for example, lets us pass data down the tree to the components that need it. However, this can present a problem.\n\nUsing props or properties is a great way to handle data. However, the deeper the component is buried, the more you have to pass the props down. This is called prop drilling.\n\n\n\nUsing the workflow above, we have a couple of nested components passing the Username prop down the tree. The Page and MainContent props are just passing down the props used by the UserCard component like a vertical game of telephone.\n\nNow combine this with having to scale out your context to include hundreds of bits of data or state, which need to be passed down to various components at various levels of the tree, and we’ve got a problem.\n\nContext in React \n\nContext solves the problem by allowing us to pass down data without relying on continually passing props through components. Context in React should be used handling global data that does not have to change often. Using context to keep track of our username state improves the workflow by allowing components to use context as needed without passing it down the tree. Pictured below.\n\n\n\nAs illustrated above, we can provide the context to one component, and the children component will be able to access the context regardless of the level they are at. All without needing to have their parent components pass the data down.\n\nContext In Use \n\nSo let’s look at an example of context. I created a small React app and just made some barebones components. The first file we should check out is the App component.\n\n\n\nThere are some lines of note, as you’ve undoubtedly seen, and then some other stuff needed for using context. The first out-of-place thing is using the React function React.createContext(), which we use to create a context object. We also made a provider for our context object and wrapped our Page component in it.\n\nContext works using Providers and Consumers. In this case, we are looking to provide the context to our app for consumption by the components. So we use the provider tag, which every context object has, to pass in the value string to our nested components.\n\nThe value attribute is a prop that the provider accepts and can pass down the tree. Many consumers can subscribe to one provider, and we’ll talk about that more.\n\nWe’re not passing anything to the Page component except the Header and MainContent components.\n\n\n\nLet’s look at the Header component.\n\n\n\nThe header is a regular old React component. However, we use the React hook React.createContent() to subscribe to the UserContext object that we import into this component from the App component. We can now use the curly brackets to pass in the userNameContext into the JSX being returned by the component.\n\nLet’s look at another example. Below we have our MainContent component.\n\n\n\nJust another component with a nested component, UserBox. Let’s look at what is in the UserBox component.\n\n\n\nThe UserBox component can do as our header did; import the context object, subscribe to the provider using the useContext hook and pass in the context using that context object. This is cool because we’re using context two levels below from where it was created without passing props through subsequent components.\n\nThe app would look something similar to the below image. We can see the username string in the header and the UserCard components. I am not one for styling things effectively, so hold your judgments.\n\n\n\nConclusion \n\nChanging the value of the state would cause a render for the other components subscribed to the context. So it could cause issues if the state is constantly changing. So context fits nicely with a global state that is not likely to change often.\n\nThe code for the above example can be found on my GitHub.\n\nThis has been an interesting learning experience. I’m happy to have gotten one of the fundamentals of React down on paper, so to speak. I hope this helps someone new coming into the React scene.\n\n-George\n",
      tags: ["React","JavaScript","Newbie"],
      id: 4
    });
    

    index.add({
      title: "Spiking Tailwind CSS in a React App",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  Tailwind\n  Setup\n  Using Tailwind in React\n  Something a Little More\n\n\nIntroduction \n\nIt’s been some time since I did any frontend development, and I don’t miss it. After a couple of years of learning the fundamentals, I would have loved to have some alternatives to manually writing CSS.\n\nNow that I am diving back into it, I am happy there are tools not to replace but improve the CSS experience. In that spirit, I want to look at Tailwind CSS.\n\n\n\nTailwind CSS \n\nTailwind is different from other CSS frameworks I’ve tried. The software works on a lower level to allow easy CSS styling utilizing class names. Bootstrap works similarly, but the difference is that Tailwind does not come with predefined components. It is also different because the class names are compiled into CSS code.\n\nTailwind is also not opinionated about how you make your designs, and thus they give you the tools you need and let you create unique components. It’s the difference between designing boots on a website and having all the materials right before you to cobble together your shoes. This alone is valuable because you can avoid falling into the Bootstrap design trap.\n\nTailwind is a “utility-first CSS Library,” From what I glean from their site, it means they tried to create a framework from a set of constrained utilities. This seems to translate into the following:\n\n\n  There are no more CSS class names to create in both HTML and CSS files as styles are implemented using low-level utility classes.\n  You add the styles you want into the HTML classes, which the compiler uses to generate CSS (which is attractive to me).\n  Your CSS files don’t grow since you’re generally not creating new styles.\n  The ability to create reusable styles using things like loops.\n\n\nTailwind also gives you ways to easily extend their utilities by utilizing config files for adding things like custom colors, fonts, etc.\n\nI’ve noticed that they seem to lean into the idea of long strings of class names in HTML over regular CSS. You’ll see what I mean.\n\nSetup \n\nSo to try this and to learn the tech better for use in my work, I created a quick React application.\n\nAfter the React app creation, we can run the below commands.\n\nnpm install tailwindcss\n\n\nThis will install the needed packages.\n\nnpx tailwindcss init\n\n\nThe above command will create the config files we need, the tailwind.config.js and the postcss.config.js files. The tailwind.config.js is where any customization options will go. By default, Tailwind looks for this file at the root of a project to create any customizations. For example, if you want to add colors or fonts that Tailwind does not have built-in, they will go in that config file.\n\nAfter that is installed, you replace everything in your index.css file with the below lines.\n\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n\n\nAnd finally, to ensure that all the template files are added to the Tailwind config, make sure the tailwind.config.js file looks like the below code.\n\nmodule.exports = {\n  content: [\"./src/**/*.{html,js}\"],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\n\n\nIt’s a bit much, but that’s essentially it. You’re now ready to start styling stuff.\n\nUsing Tailwind in React \n\nAfter setting up our React project and installing Tailwind, I was ready to go. So I got rid of all the startup React stuff and started small.\n\n&lt;h1 className=\"\"&gt;I'm using Tailwind!&lt;/h1&gt;\n\n\nPretty easy, and we get a simple heading tag.\n\n\n\nNow let’s start small and add some styling.\n\n&lt;h1 className=\"text-red-700 text-6xl hover:text-9xl\"&gt;I'm using Tailwind!&lt;/h1&gt;\n\n\nNow I added a couple of styling classes to the JSX, and just like we were editing a CSS file, we got some results.\n\n\n\nYou may also notice the hover selector in there. Tailwind takes care of these, similar to how CSS does. You prepend the effect you want and the outcome, and it works just the same.And we can see the style change a little when we hover over the text.\n\n\n\nAdding these class names saved me from opening up VSCode and adding styles to a CSS file. I am already sold on Tailwind.\n\nYou can also see the core use of Tailwind in adding class names to the HTML tags. This is a small example, but tags can have tons of styles, so adding a class name into the HTML can get overwhelming quickly. This is the language they lean into that I mentioned above.\n\nSomething a Little More \n\nI am not a designer, but I find this setup easy to create components. So let’s say I broke my app into pieces. How can I style this card component I made? Tailwind makes it simple.\n\nexport default function Card() {\n    return (\n        &lt;div class=\"p-20 bg-green-100\"&gt;\n            &lt;h3 class=\"text-green-300 mb-4 text-sm font-bold\"&gt;\n                This is some cool Tailwind Stuff!\n            &lt;/h3&gt;\n            &lt;div class=\"border-4 border-green-800 bg-white p-6 rounded-lg shadow-lg\"&gt;\n                &lt;h2 class=\"text-2xl font-bold mb-2 text-gray-800\"&gt;Look at this!&lt;/h2&gt;\n                &lt;p class=\"text-gray-700\"&gt;We did some Tailwind!&lt;/p&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    )\n}\n\n\nAnd the results.\n\n\n\nI didn’t have to write a single bit of CSS for this, and now I have a perfectly usable component. There’s no ending to this rabbit hole. Design all you want.\n\nConclusion\n\nI can’t bring myself to write CSS. It’s a doomed relationship; with too much bad blood and too much history. However, I might just get through with Tailwind as a buffer for those awkward times I have to sit with it.\n\nHyperbole aside, Tailwind is not a replacement for CSS but a fantastic addition to CSS for easily styling web components. Coupled with React, this was how we were meant to make apps. I’m excited to continue learning and hope this helped.\n\nSmall disclaimer: I am not suggesting anyone reading this who might be new to frontend development jump straight into learning Tailwind. That journey starts with learning how CSS works. Much like filmmaking, learn all the fundamentals first and then break the rules at your leisure.\n\n-George\n",
      tags: ["React","Tailwind","JavaScript","Intermediate"],
      id: 5
    });
    

    index.add({
      title: "Using SWR for Easy API Calls in React",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  SWR\n  Conclusion\n\n\nIntroduction \n\nReact gives us the ability to create powerful and scalable apps. We need to quickly implement API calls to get our data displayed in our App for them to run. That’s where SWR comes in handy.\n\n\n\nSWR \n\nMy company embarked on a journey to create a new app for stakeholders. This included React for the frontend. Since then, I’ve been working on some React side projects to reinforce my learning from work. I struggled to use some built-in hooks to no avail. Then came SWR, which was mentioned as a way to do API calls in our work app. After integrating SWR into one of my side projects, I am sold.\n\nSWR is a react library used for easily fetching information from an API endpoint. SWR stands for “stale while revalidate.” According to their site, they derive the name from the practice of invalidating the cache. They use the cache to return data then send the fetch request to get up-to-date data. This makes sense seeing how the library seems to fetch data without user intervention.\n\nI was impressed seeing what is essentially a stream of data on my app as the data seamlessly updating when adding data. Plus, it’s lightweight and easy to implement. It also boasts some features over the built-in useEffect hook as it also passes in errors for better exception handling and a more seamless experience maintaining the code.\n\nExample \n\nI’ll take us through how I implementedSWR it into my app. I already have a  React app, and I ran the install for SWR detailed below.\n\nnpm install swr\n\n\nOnce that finished installing, I went to work. The first thing was to import the useSWR function into one of the components.\n\n\n\nNext was a simple fetcher function, taken right out of their docs.\n\n\n\nSome notes about the fetcher. SWR takes in any fetcher object you want to pass to it. You can utilize libraries like unfetch, Axios, or GraphQL. I used the native fetch as I don’t have any unique logic running, and I am only returning simple JSON objects. So I configure it to use JSON for returning the data.\n\nSWR has three states for the data returned, Loading, Ready, and Error. These states can be used for exception handling or creating logic for loading behavior.\n\nThe last thing we want to do is use the useSWR function we imported to grab our file.\n\n\n\nThere are a couple of things of note within this call. First, we’re passing in the fetcher function we created earlier and the API endpoint. Secondly, we set the results as an array with data and error since we get back error data if we error out and want to store the message. Our returned data will go into the data variable.\n\nThat’s easy. React treats the return values as an object. The data can be accessed using a map() function and the  data.&lt;tag&gt; syntax.\n\n\n\nConclusion \n\nAfter some documented time experimenting, I was resigned to React’s perceived complexity. However, as I build out more and more small accomplishments, I realized React is like anything else; it takes practice and the right tools. SWR is something I’m putting into my React utility belt. I hope this helps someone who might be struggling.\n\n-George\n",
      tags: ["React","JavaScript"],
      id: 6
    });
    

    index.add({
      title: "Table Resource VS Client in AWS",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  The Problem\n  The Answer\n  The Answer\n\n\nIntroduction \n\nDynamoDB provides an excellent way to store data in a flexible and scalable way. Add it to Lambda and API Gateway, and you have a powerful group of tools, which I have written about. It was for these reasons that I chose to use it for a side project I am building to familiarize myself with React. It was in these features that I struggled with an issue in how boto3, the SDK for talking with AWS in Python, implements their libraries.\n\n\n\nThe Problem \n\nI have a personal rule; if it takes a certain amount of time to debug an issue, then there is something about the underlying technology that I do not understand. As it happens, I spent some time trying to get the APIs to work correctly in my side app recently. The problem boiled down to how my return data from the API was displayed in the application. It confounded me for some time. So I stepped back and challenged my assumptions about the tech. I followed the rabbit hole back to the API I wrote, where I found the problem.\n\nIt turned out the problem was in using the Client class in boto3. From their docs, Amazon calls the Client class a “low-level client representing Amazon DynamoDB.” So this made sense when I started seeing how the API would interact with my app.\n\nWhen making a call to the API, the JSON returned would have all these extra tags on them. Below is an example.\n\n\n\nIt seems that DynamoDB was tagging the data types, as string types would have the type “S” for string values and “N” for number values. It made little sense to me. There are possible solutions available in the library that will help unmarshal the data and clean it up a little, but these tools are buried in the source code for boto3 and don’t seem to have official documentation. There’s a yet unresolved GitHub issue about it. So I had to look deeper for a solution.\n\nThe Answer \n\nAfter a while of searching through the AWS docs and trying various solutions, I was able to come up with a fix. I rewrote my API to grab all the records using the Table class rather than the Client class.\n\nIt turns out that the Client class works precisely like a low-level abstraction should, returning all the stuff that’s stashed into DynamoDB data tags and all. Not very reader-friendly.\n\nHere you can see what the code looks like using the Client class.\n\n\n\nAnd here is the updated code for the Table resource.\n\n\n\nVery little in the logic changed. We are still creating a connection to AWS using a Lambda function. We are still returning the response to that function.\n\nThere are, however, a couple of differences. We call the Table resource a little differently from the client, and I added some pagination to the table code. As boto3 will only return everything up to 1 MB, you need to put in pagination to get all your results.\n\nMaking that change makes the API returns a cleaner response.\n\n\n\nNo data tags and no messy nesting in our API data anymore.\n\nConclusion \n\nI like challenging my assumptions. Being outside my comfort zone is how I learn best. I’ve been making small scripts similar to that using the Client class for a while now, so it was good to understand the limitations in the tool and the tradeoffs using others. How this discomfort shakes out in the React app remains to be seen, but I am sure I’ll figure those problems out too. I hope this helps someone.\n\n-George\n",
      tags: ["AWS","Lambda","Intermediate"],
      id: 7
    });
    

    index.add({
      title: "Modules in React",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  React Modules\n  In-App Example\n  Named Imports vs. Default Imports\n  Conclusion\n\n\nIntroduction \n\nThe organization of your apps is one of the most effortless quality of life improvements you can make. Recently I’ve been deep-diving into React for a work project, and I was able to get a crash course in how you organize one such app. Including how React treats its files and how we’re importing them. So today, I’d like to get into how React handles modules and how importing is done.\n\n\n\nReact Modules \n\nReact has no opinions on how you organize your code. This is fine as engineers usually have plenty of views on this, and the subject matter expert we’re working with was no exception. After talking it through, we decided to organize our app where each feature was organized into its own directory. This also gives us a choice to utilize index.js files to handle exports.\n\nThe most straightforward analogy coming from Python was that React lets you put your files into a directory and then create an index.js file that exports everything. This is similar to how Python will utilize an __init__.py file to export everything into the main logic files.\n\n\n\nSo if I have one or more modules I need to import into the App.js file for a feature, I can utilize the index.js file to import all of them and serve as one export point for your App.js or wherever you are utilizing said feature.\n\nIn-App Example \n\nLet’s go through a quick example. I created a small React application to demonstrate what I am talking about. I organized it as such.\n\n\n  Src\n    \n      App Feature\n        \n          Hello.js\n          Worlds.js\n          Index.js\n        \n      \n      App.js\n    \n  \n\n\nI created a directory named AppFeature to hold the modules I want to organize under there. The Hello and World modules look similar; they are only components that render a new div tag with some words. See the code below.\n\n\n\n\n\nThe AppFeature directory will have an index.js file that will handle exporting. This file will be used as a “central hub” where we can export all our modules into different parts of the application. You can see the code below.\n\n\n\nIn the code above I put in two export statements. From here is where our modules will be exported from. This makes for cleaner imports into our App.js file. As seen below.\n\n\n\nAt the top, you can see a clean import statement where we import a list of modules from the AppFeature directory. After that, we can apply them right into our application. It comes out looking like this.\n\n\n\nNamed Imports vs. Default Imports \n\nThe above example details what are referred to as Named Imports, but that is not the only way to handle exporting and importing. There are other ways to export/import your needed modules. For example, suppose we are building components with multiple modules that do not need to be imported into the main application files. In that case, we can organize them so our main module can import all it needs and serve as the main component or class module to be exported. As demonstrated below.\n\n\n\n\n\n\n\nIf we organize our code such that we want to keep internal modules internal, we can explore this type of organization. Using Default Imports, we’re still doing the same thing: we use our index.js file to handle our exports. The difference is that we’re organizing everything into one module and only exporting that one module. This makes everything even cleaner.\n\nConclusion \n\nOne of the goals for the application we are building is to optimize our workflow. Organizing your code by utilizing modules in React is just one way in which we are meeting that goal. There are many things to consider, but I believe we are headed in the right direction. I hope this helps someone looking to organize their React projects.\n\n-George\n",
      tags: ["React","Newbie"],
      id: 8
    });
    

    index.add({
      title: "Implementing Routing in React",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  React Router Dom\n  Setup\n  Explainer\n  Conclusion\n\n\nIntroduction \n\nImplementing routing in React applications can be one of the first tasks you undertake in the coding part of the development lifecycle. This was true for my team when we sat down to a mob programming session to go through a couple of our stories for a project. We brought in some help from people who know React to help us get started. I am not a React developer, so this is the first time I had actual exposure to people with expertise, so I am grateful it was a group programming session. I don’t know what I was doing when the rest of the world learned React, but I can only guess it had something to do with playing the Sims. Regardless I use the right tool for the job, and I need to learn it now. The first thing we did was implement some routing, and I will implement a much more simplified version of what we did below to show what I learned.\n\n\n\nReact Router Dom \n\nI understand React is a set of APIs and libraries used in different ways to implement cool stuff on your screen. We got more into using TypeScript on top of React, but I like to write about things even if I only have a cursory knowledge of them, and my understanding of TypeScript doesn’t even measure up to that. So I’ll stick with React only for now.\n\nRouting in a React app is what the app does when a user goes to a specific URL. As we’re dealing with React, we need to create components that make up our pages, and we’ll use a library to route to those components. In our case, we used the ever-popular React Router Dom.\n\nSetup \n\nAn easy bit of setup. I created a new app using npx create-react-app react-router, which gave me a basic app. I then ran npm install react-router-dom to install the needed package. Then I was off to the races.\n\nI created two simple page components to then import into my main App.js.\n\nThe Home component.\n\n\n\nAnd the About component.\n\n\n\nFinally, I went through some of the docs and found an easy way to get basic routing. All of which is reflected in my main App.js file.\n\n\n\nExplainer \n\nThis needs explaining. I created a basic component for the home and the about pages. Both of them just return a header with the name in the tag. I imported them into app.js and imported BrowserRouter, Routes, and Route from the React Router Dom package. Each of these is going to help us create routing.\n\nFirst, we create the router using the BrowserRouter tag. Then nested in there, we make the Routes block. The routes block, which took the place of the Switch block in v6 of the React Router Package, looks at our nested routes and tells the app where to go. Finally, we have the “links” in the Route tags. We specify the path to look for in the Route tag and which element to point at. It’s also good to remember that the element should take the form of curly brackets and an open and closing tag (the {&lt;Home/&gt;} you see in my code). I mention this as most tutorials I looked up about this used Switch, Links, and the elements used the {Home} syntax.\n\nAnd that is it. If I navigate to localhost:3000/, we see the below image.\n\n\n\nAnd the same thing on the about page at localhost:3000/about.\n\n\n\nConclusion \n\nIt is a simple thing but something I learned while working with a team of React people. Using React is still akin to reading an upsidedown French map of Germany. However, I am starting to grasp the basics. Everything seems just to be something that’s been imported from somewhere else. Next time I think I’ll write about how you can create React packages similar to Python for easy and clean importing.\n",
      tags: ["React","Newbie"],
      id: 9
    });
    

    index.add({
      title: "Dockerizing System Tests With Selenium",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  Selenium WebDriver\n  Setup\n  Running Tests\n  Notes\n  Conclusion\n\n\nIntroduction \n\nWe are spinning up a new application for some end users to enter data, so we need to build a robust testing system. Unit tests are a must, but those only test to ensure that all the classes, methods, and functions do what we expect. On top of that, we also need to verify that the web app as a whole looks and behaves how we hope it does to have a complete end-to-end testing apparatus. My goal for these first few spikes in the project was to find a tool for system testing and see if we could make it modular and easily automate it to not interfere with our workflow. I believe I found the solution. Selenium is a suite of tools for creating and automating browser tests, and I think it is what I was looking for. Let’s dive in.\n\n\n\nSelenium WebDriver \n\nSelenium is a multi-language suite of tools for creating browser-based automation tests. This tool will automate opening a browser, looking for a test condition, and reporting a pass or fail. Selenium is a whole suite of tools, but I decided to focus on Selenium Web Driver.\n\nMuch like in the name, the software “drives” the web browser. Much like humans, we can bring up the webpage in a browser, enter in data, look up tags, and utilize all these capabilities to write tests using the API Selenium provides. A test opens a browser, and like magic, many things happen.\n\nMy goal is to automate as much of this process as possible. So it would not be ideal to have a bunch of browsers launch with with ghosts running tests. A better solution was to run headless browsers using container images provided by Selenium. This solution gives us a container with a built-in web browser which we don’t see that our script runs a test against. The results we can see in the terminal.\n\nAs our project utilizes Docker, the easiest way to start is to grab the Docker image for the browser and start making some scripts. How this will look in the final solution is still to be determined, but I can replicate what I did for the sake of this write-up.\n\nSetup \n\nTo begin, I set up a docker-compose.yml file with the following:\n\n\n\nThis gave us two containers, our browser using the Chrome-based selenium image and a Ruby-based container to run the script. Our new stack uses a lot of Ruby. The system container is built using an image from the docker file in the test directory.\n\n\n\nThis build handles installing the gems we need, including selenium-webdriver and the chromedriver-helper. Both are used to run our Web Driver script and utilize Chrome capabilities.\n\n\n\nFinally, the last bit is the script itself.\n\n\n\nRunning Tests \n\nLet’s look at this script. My Ruby is rusty, but I tried my best. We set all our requirements, set a timer to make the thread sleep (more on that later), and then we write our test. In the script, we are writing the tests using the RSpec. RSpec is a domain-specific language built using Ruby to test Ruby code. We use this to test our behaviors using the describe and it blocks.\n\nWe start by defining the capabilities we are looking for; in this case, we are testing a chrome browser, so we need to specify that.\n\nThen we use a variant of WebDriver called the Remote WebDriver. Remote Driver is written identically to WebDriver, just with the caveat of the driver logic looking for the browser in another system. Here we set the address for the Chrome Selenium container so that our WebDriver knows to look for this remote machine to run the test against.\n\nBoth containers are in the same network provided by Docker Compose, so we use the hostnames. Also, note that we are mapping port 4444 as the WebDriver will use this port to communicate.\n\nWe then set the driver to navigate to our chosen website as an action. The following line sets what we expect to see using RSpec’s handy expect function. We expect the page’s title to be equal to a string we provide and fail if the title is mismatched. Then we’ll just take a screenshot and save the image to a local drive using one of the built-in save_screenshot functions. Finally, and this was important, use the quit function.\n\nWe run this just by running docker-compose up, and we can see the test passed as going to Google does indeed yield a page title of “Google.”\n\n\n\nWe can also see the screenshot taken from the remote browser.\n\n\n\nNotes \n\nUsing the quit function was essential to kill the connection, but it also closed the browser we can’t see. Additionally, I mentioned I would come back to using the sleep function in Ruby. It turns out using the depends_on feature in Docker Compose is not enough to ensure services are available to each other. When I began running the network, the remote driver would continually fail to connect. It turns out we just needed a moment for everything to boot. Docker recommends creating a wait script but pausing the thread for a moment worked just as well.\n\nConclusion \n\nThis was a pretty simple example, but it answered my questions. We can use this to test website behavior, make it modular, and automate the tests. That checks enough boxes for me to keep going down this rabbit hole. The next goal is to develop an automated solution and possibly clean up the deployment a little. I’m thinking of having these two containers not run with the rest of the project and boot up the containers using a bash script specifically for testing. I might write about that too.\n\n-George\n",
      tags: ["Docker","Testing","Intermediate"],
      id: 10
    });
    

    index.add({
      title: "Shared Memory in Docker",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  Shared Memory Device\n  Use in Docker\n  Conclusion\n\n\nIntroduction \n\nRecently, I was experimenting with system testing on a new stack. While working with Selenium Remote Driver to create headless web browsers (more on that later) to run against my app, I came across a Docker option I had not known. This Docker feature allowed for easily accessible and fast file reading and writing between the host and the container. It was being used as a volume, but I learned it was much more. When I looked it up, it was referred to as a Shared Memory Device.\n\n\n\nShared Memory Device \n\nTurns out that this is an old Linux staple. The shared memory device, located in the /dev/shm/ directory in the file system, utilizes temporary storage using RAM rather than disk storage. Using a shared volume available on the RAM and not the disk makes for a much faster read/write speeds. These devices also allows for inter-process communication.\n\nUse in Docker \n\nDocker allows for the use of this device. When conducting my testing, I used the volume mapping feature to easily map local directories with directories inside the containers we’re working on. As seen below.\n\ndocker run --rm -it -v /dev/shm/:/dev/shm/ --name ubuntu ubuntu\n\n\nThis mapped the local shared memory device to the Docker device. Docker, by default, allows for up to 64 MB of storage space using the shared memory device. You can see the default allocation when searching for the shared storage device within the container. As seen below.\n\n$ docker inspect ubuntu | grep -i shm\n            \"ShmSize\": 67108864,\n\n\nI was saving screenshots just to ensure that I was looking at the thing I was hoping to look at. Although I was only doing some foundational work, I could find myself in a pickle with only 64 MB of space available if this work ends up scaling. Should we need to, Docker lets us change the size of this device using the –shm-size option. As seen below.\n\ndocker run --rm -it -v /dev/shm/:/dev/shm --shm-size=2gb --name ubuntu ubuntu\n\n\nAbove we add in the –shm-size option and we can specify all the space we need! Let’s look one more time to confirm that it works.\n\n$ docker inspect ubuntu | grep -i shm\n            \"ShmSize\": 2147483648,\n\n\nShazam! We have more storage in our shared memory device to play around with.\n\nConclusion \n\nThere is more to come about my adventures in Dockerizing system testing, but for now, I was able to learn a new thing about Linux and Docker. I hope it helps someone who came across it without knowing as I did.\n\n-George\n",
      tags: ["Docker","Intermediate"],
      id: 11
    });
    

    index.add({
      title: "Using Amazon API Gateway with Lambda",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  Amazon API Gateway\n  Setting up an API\n  Adding the Trigger\n  Setting the Methods\n  Conclusion\n\n\nIntroductions \n\nI follow this movie podcast where they review and talk about well movies. It’s a fantastic podcast with a diverse set of people who’ve worked in the industry a long time, and I enjoy their opinions. At the end of each episode, they do a “Staff Pick” where they pick out a movie that shares themes with the movie they just reviewed or movies they think people should see. As I don’t have time to watch all the movies they suggest every week, I need to keep a running list of these films. They offer some good stuff.\n\n\n\nThis presented a good opportunity to get some practice using React. I could build a tiny site cataloging all the suggested films. I have no React knowledge, so I started by creating a database to store their suggested films to stick with the iterative approach. Now I need to set up something to query the database and get back a list of the movies. That is where Amazon API Gateway and Lambda come in.\n\nAmazon API Gateway \n\nI’ve covered AWS Lambda before, so I won’t go into it much. Amazon API Gateway, however, is an AWS service that allows you to quickly spin up API endpoints for use in your web applications. When you need to request data for your website, an API endpoint is what you’ll use. An API endpoint is a URL that your application can reach out to request some data. For example, this API endpoint lets me look up Studio Ghibli movies.\n\nThese can scale like crazy, and API Gateway lets you manage, monitor, and add endpoints to your service quickly. For this project I am spinning up, we only need to return a list of the movies in the table. So let’s begin.\n\nSetting up an API \n\nThe first thing is setting up something to process the request once our service reaches out via an API endpoint. AWS Lambda is what we’ll use to create the process to query our database and return all our stuff.\n\n  So we go into the AWS Console and create our Lambda. We’re returning all our items with a simple query, so it’s an easy setup. Here’s the code:\n\n\nimport boto3\nimport os\n\ndef lambda_handler(event, context):\n    client = boto3.client(\n        'dynamodb',\n        aws_access_key_id=os.environ.get('KEY'),\n        aws_secret_access_key=os.environ.get('SECRET')\n        )\n\n    def get_all_items():\n        \n        response = client.scan(\n            TableName='MaxFilmStaffPicks',\n        )\n\n        return response\n        \n        \n    return get_all_items()\n\n\nAdding the Trigger \n\n\n  After we have the Lambda function up and running, we need to create something to initiate it, like a trigger.\n  You can see a button to Add trigger at the top of the Lambda menu. You can click that and select API Gateway in the proceeding menu.\n\n\n\n\n\n  Then you can hit Create API, which brings up the options for APIs. In this case, I’m making a REST API, so I select that.\n  For security, I pick Open, as this is just a practice app. You can see what I chose below.\n\n\n\n\nSetting the Methods \n\n\n  I finish off by going into the API Gateway menu to check out the newly created endpoint.\n  After that, you’ll be greeted by a screen with all our methods. The default created method is the ANY method; I want this to be a GET method. The difference between HTTP messages is a bit out of scope, but you should know them. We are using this endpoint only to request information for our application, so we stick to GET.\n  We’ll select the ANY method, click the Action menu above it, and delete the method.\n  Then we go back to the same menu and click Create Method.\n  From the dropdown, we select GET.\n  Following this, we enter the name of the Lambda function this is for, and we’re done.\n\n\n\n\nVery last step is to deploy the API.\n\n\n  We click the same menu and click Deploy API.\n  You’ll then be asked which Stage you want to deploy in. When creating APIs, you can create many stages for your APIs, which will affect how they are used—for example, setting up a dev stage or creating different versions of your API. Whatever stage you set will reflect in the endpoint. See below.\n\n\n\n\n\n  For this instance, it is one API endpoint, in one app, for one specific purpose. I just used the default.\n  Now we’re done. We test the endpoint, and we get data!\n\n\n\n\nConclusion \n\nNow I have a working API for grabbing my data. React confuses me, as I thought I’d be able just to pull data like I would a Python app. However, I am sure I will learn much more as I continue. And now you also have some knowledge of setting up endpoints in Amazon API Gateway.\n",
      tags: ["AWS","Lambda","API Gateway"],
      id: 12
    });
    

    index.add({
      title: "Shenanigans with Shaders",
      category: ["Graphics"],
      content: "Table Of Contents\n\n  Introduction\n  Shaders\n  Setup\n  Shader Code\n  Conclusion\n\n\nFor those of you who love rabbit holes, learning graphics programming is a pretty deep one. There’s always some new thing to learn, there’s a bunch of different new languages and toolsets to know, and on top of all that, there’s math. Like anything else in programming, you pick up momentum with each new thing you build, so I found a tutorial and started making shaders. I know very little about this. However, I’m writing what I’m learning, so don’t come for me if I’m off on anything.\n\n\n\nShaders \n\nA shader is a program that runs on the GPU as part of the graphics pipeline. We’re going to focus primarily on shaders in Unity. There are other ways to tackle this, but Unity gives an easy setup to get started quickly. For the context of Unity, a shader is a small script containing logic and calculations for determining the colors of a pixel.\n\nIn Unity, we create shader objects which act as wrappers for our shader program. A shader object exists in a shader asset which is just the script we are writing. Creating these in Unity allows for a great deal of freedom in what we make. What we’ll focus on is adding some basic functionality to a shader. We’ll be focusing on using ShaderLab to create shaders.\n\nSetup \n\nThe first thing to set yourself up making shaders in Unity is Unity. So download it, and create a new project.\n\n\n\nI won’t give a full rundown of Unity and the stuff you can do. I leave that to better minds. In the Hierarchy Window, right-click and scroll to 3D Object and click whichever object grabs your fancy. I always pick sphere for testing stuff. Now we have a 3D Mesh on the screen that we can begin adding things to it. In the Project Window, right-click on the word Assets and create two new folders, Materials and Shaders. Double click into the Materials folder, right-click and Create is right at the top -&gt; click Material. Materials are similar to skins we can apply to 3D objects. We will use this new material to add our new shader to the 3D Mesh. After that, drag our new material into the Scene Window where our sphere is and onto the sphere we made. Now right-click our Shaders folder scroll to Create -&gt; Shader -&gt; Standard Surface Shader. Click the sphere in the Scene window to bring up the Inspector Window. Finally, drag the shader file over to the inspector window with our sphere covered in our new material. We have just applied our shader to the materials. You should see this in the Inspector Window.\n\n\n\nNow go back to the Project window and double click our new Shader file. Unity will launch an IDE for use to check out the code. You can configure your choice of IDE; I have VSCode configured. Open the Shader file, and let’s check out the code. I created some basic shader code you can use.\n\nShader Code \n\nHere is the complete, minimal shader code:\n\n\n\nIt looks a bit much to anyone new to this, including myself, so let’s take it a section at a time. The first thing at the top, starting with “Shader,” is the Shader Block. This is used to define our Shader Object. You can use this to define your properties, create many shaders using the SubShader blocks, assign custom options, and assign a fallback shader object. Here you can see the name of our shader and that it is in the “Custom” directory.\n\nWithin the Shader block curly brackets, we have our other sections. The first is our Properties. The properties box is where we define the properties for our materials. A material property is what Unity stores along with our materials. This allows for different configurations within Unity by creating things like sliders and inputs within the Inspector window for us to play around with. We defined two properties, the MainColor and the MainTexture. Using square brackets, I outlined which property was the default color and default texture. We also defined the default values for these properties. There’s a bit to these values but suffice it to say, both values are default white.\n\nThe second block is our SubShader; this is where our shader logic goes. You can define multiple sub shaders for many different uses. For example, depending on the graphics hardware you want to support, you can make shaders for the various graphics APIs. Within our block, you can see some code for assigning tags, assigning levels of detail (LOD), and the CGPROGRAM block. I want to draw your attention to this section of the code:\n\n\n\nFirst, we define the data types for our inputs and outputs and create a function for us to serve the outputs into unity. Our Input we set up as uv_Maintex; this allows for us to input a texture object. Then we create a fixed4 variable for our _Color attribute. The o.Albedo parameter is what is used to control the base color of the surface. Here we are taking the values of our texture and multiplying them by our color input. The code above gets you something similar to this:\n\n\n\nI was proud of myself the first time I made this from memory. Our coded shader lets us control the color of the material and add basic textures to it. Working in graphics does not lead to instant gratification, as anything you do requires a ton of setup. However, this and ShaderToy get you that dopamine hit.\n\nConclusion \n\nAbove I went through some fundamentals of shaders in Unity. I skipped over a ton of information as I’m still learning a lot, and a full informed explainer would be twenty pages long. There is a lot to programming graphics and shaders specifically. I suggest you check out stuff like Team Dogpit’s shader tutorial for a way better deep dive. I’m excited to dig into this world. I want to learn to create some of the incredible stories I see in animation, and any first step is a step in the right direction. Thanks for reading.\n\n-George\n",
      tags: ["Unity","ShaderLab"],
      id: 13
    });
    

    index.add({
      title: "Messaging and Madness: Sending Messages with AMQP and Amazon MQ",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  AMQP\n  AMQP and Amazon MQ\n  Serialization\n  Conclusion\n\n\nIntroduction \n\nHow do software systems talk to each other? Back-end systems can scale into giant melted together Cronenberg monsters, often making up different tools and languages. So, communicating between these services can become an untenable challenge without some shared vocabulary. We can communicate in many ways, but today I wanted to talk about asynchronous messaging protocols and figure out how AWS can help.\n\n\n\nAMQP \n\nAMQP stands for Advanced Message Queuing Protocol. I’ve been working to implement it for some back-end software suites I’m building out to enable them to talk to each other. AMQP utilizes these things called brokers to publish messages on, then on the other end, a receiving service subscribed to the same “channel” that we posted to can pick up that message.\n\n\nvia Rabbit MQ Tutorials\n\nLet’s dive a little further down; the publisher service publishes a message to an exchange on a broker. This exchange has routes that lead to queues, or “channels,” where the payload is published. We make sure to include the sending information with our message to be routed to the correct queue. The broker cannot see the message, although it might look into any metadata attached to the message from the publisher. This workflow asynchronously sends messages. Imagine a server version of a mail sorting machine shooting letters into the correct mail slot based on the address.\n\n\n\nWhen referring to a publisher, I mean some code that we utilize to connect and send a message. AMQP is programmable, so I can shape it to fit most situations. In this case, we need to send messages to our different software suites to trigger actions to happen. Learning this took some time, but it’s been simple to implement.\n\nThere are different types of exchanges that we can use to make these services fit our needs. I’m going to explain what we use briefly.\n\nWe use a direct exchange utilizing routing keys to bind queues to exchanges. Our code can use direct exchanges to distribute tasks to many different endpoints, but we used these direct exchanges to make direct routes between our services. Other types of exchanges can be used to broadcast messages. More information can be found here. For now, we’re going to focus on direct exchanges.\n\nAMQP and Amazon MQ \n\nWe touched on all that because I wanted to talk about Amazon MQ. Amazon MQ is a fully managed platform for setting up message brokers. Amazon MQ utilizes both RabbitMQ and Apache Active MQ for creating brokers. We’re sticking with Rabbit MQ for the time being.\n\n\n\nHere above, you can see you can easily set up a broker in just a few clicks. I left most of the settings on default, except for choosing “RabbitMQ” for our broker engine and setting some security up for accessing our management console.\n\n\n\nOnce we get that, we have access to the RabbitMQ dashboard Amazon MQ created and is managing. Now that we have a broker set up, we can play with some code.\n\n\n\nAbove I use the library Kombu to create some connections and send some stuff. I started by setting up our environment variables. Then created exchange and queue objects. Finally, I made our connection object and the producer object, and then we sent a simple “Hello” message.\n\nSerialization \n\nSerialization is another blog post, but I chose to use JSON to serialize the payload. In the production software, I use a combination of JSON and Pickle to serialize things like image data.\n\nNow we can see our message published on the queue I declared in our publisher service. An identical receiving service would be set up on the other side to read out messages sent to that queue.\n\n\n\nConclusion \n\nIn conclusion, using Amazon MQ allows us to set up managed brokers for us to send messages. With AMQP as the broker engine, we have a lightweight message-sending workflow. Thanks for reading.\n\n-George\n",
      tags: ["AWS","Python","AMQP"],
      id: 14
    });
    

    index.add({
      title: "Health Checking S3 and DynamoDB in AWS",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  Problem\n  S3 Solution\n  DynamoDB Solution\n  Conclusion\n\n\nIntroduction \n\nA hybrid infrastructure has tons of exciting challenges. Although we host a great deal of our software in AWS at my company, we cannot do everything in the cloud. As such, we have tons of physical infrastructure as well. This hybrid infrastructure presents many challenges that we strive to overcome on the software team. One of the challenges we are working towards is imaging and utilizing software to detect our yields. This piece of that puzzle will focus on storage for our images.\n\n\n\nWe decided that we would use a combination of services offered by AWS. The first is the Amazon Simple Storage Service or S3 for image storage and DynamoDB for holding metadata of said images. Given that we are getting information straight from hardware, many things might go wrong, from getting the pictures to when said pictures are pushed to AWS. This brings us to this evening’s question: How can I be sure these services are available for me to send stuff to?\n\nProblem \n\nWell, as it turns out, there are a few ways this can be done. For example, there are libraries out there that will scan health check websites to see if AWS has any service outages. This would not be a great way to do health checks for a production application. So, I decided to spike this problem and make something myself. I am not worried about AWS services being out as they have high availability using their different availability zones. I am more concerned about our endpoints failing, internet issues, or Cloverfield monsters. So, this needs to be explored.\n\nS3 Solution \n\nA simple solution for checking the health of my resources was needed. Luckily, I quickly put something together using the Boto3 library, which is the AWS SDK for Python. This library gives us easy access to the AWS API for configuring and managing services. The first thing I did was create an object class to utilize the Client class in Boto3.\n\n\n\nWe only need to pass in our access credentials and the services we want to create a client object for, and we get our client object back. Each turn in Boto3 allows for interacting with the Client class. The docs define the Client class as “a low-level client representing whatever service”. In most cases, you would use it to access the various functions for interacting with the service.\n\nAfter that, I put together some simple logic to return some information on the resource we are looking for. In our case, we were trying to get access to a bucket where we will store images. This solution is enough to satisfy me that the resource exists, and I can communicate with it. Below is the code I used for S3.\n\n\n\nThe code above sets up a new client instance and utilizes the head_bucket() function. This is great for seeing if a bucket exists and if the entity polling it has permissions to access it. In my case, I only need to be able to see if I get a message back. So, I pass in the bucket name, and I can receive a 200 message back from the server if the resource is there and I have access to it. I like this approach because it is dead simple, and I also get to utilize the custom exception that we get access to using the client object, which is the NoSuchBucket exception. Using this exception allows us to be concise with our exceptions.\n\nThere were some questions about the limitations on being able to use something like this. We expect to use this frequently to pole S3 and make sure that we can talk to our bucket. If AWS is not available, we need to turn off the spigot and stop our software from sending stuff to AWS and not lose messages in the void of space. That said, we will be polling a few times a second at least; luckily for us, S3 upped their request rate to 3500 to add data and 5500 for retrieving data. This gives us plenty of room to be able to pole what we need.\n\nDynamoDB Solution \n\nWith the client object that we created above, we can also use that to access DynamoDB. As such, the code is below:\n\n\n\nThe above code snippet does the same thing as the S3 code does. We create a new instance, and we use the describe_table() function while passing in the table name. This function returns information about the table, including the status. Also, note that the ResourceNotFoundException is another custom exception provided by the Dynamo Client object. This bit of code satisfies what I need to be able to check the status of a table. Yay!\n\nUsing this method also has similar challenges. The decribe_table() function uses up an eventually consistent read on your table. So, getting out-of-date data is possible if you are polling something you just created, so give it a second. If you are using a provisioned table in Dynamo, this method will take up one of your reads per second. We will need to make sure this is accounted for when we start designing our database.\n\nConclusion \n\nThe above simple bit of code was a brief spike for a solution we needed to explore. This write-up was inspired by a lot of the help I received from my fellow AWS Community Builders. Checking the health and status of services is one of many things that we will build out using AWS. I am excited to keep up my learning and building. If you have seen or made other stuff to accomplish this type of work, let me know! I would love to learn more.\n",
      tags: ["AWS","Python"],
      id: 15
    });
    

    index.add({
      title: "Encrypting Your Environment Variables in Lambda with KMS",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  Key Management System in AWS\n  Customer Master Keys\n  KMS and Lambda\n  Conclusion\n  Additional Notes\n\n\nIntroduction \n\nDo you hate when gnomes steal your underpants for profit? I know I hate when those guys come out to steal my stuff. Unfortunately, I cannot help you prevent the theft of your undergarments, but I can help you protect some assets in AWS. Specifically, we are going to talk about encrypting environment variables in Lambda.\n\n\n\nIn a previous article, I talked through about how to create a Twitterbot using AWS Lambda. I mentioned that we would talk about encrypting environment variables in my last AWS article, and this is it. Encrypting stuff is a big topic and crucial for maintaining a secure environment and not just in Lambda.\n\nWe all know what encryption is, an ancient method for converting information into a secret code that only the correct parties would have access to. Usually, this is in the form of a key which both parties have access and can use to encrypt and decrypt. Anyone else looking at it would not be able to tell the difference between it and white noise. Records of civilization using encryption go as far back as Egyptians in 1900 B.C. using it to encode messages on the walls of tombs. Our modern solution for this ancient technique requires us to dive into yet another service that AWS offers, Key Management Services, or KMS.\n\nKey Management System in AWS \n\nKey Management service, in short, is a service that allows us to manage encryption keys for various AWS services or within your AWS applications. KWS gives you a central repository to easily create, manage, and rotate your encryption keys. If you are wondering, the act of rotating encryption keys is when you generate new keys, re-encrypt all the data using the new keys, and then delete the old keys. It is an essential service in AWS.\n\nKMS is considered a multi-tenant hardware security module or HSM. It’s a blade server sitting on a rack that handles the numerous clients managing keys in AWS every day. The hardware is created in a way that many clients can use the hardware but still be virtually isolated. The keys are stored in memory and not written to disk as a security measure. This way if the hardware is powered down the keys are gone and thus inaccessible.\n\nAWS does provide a single-tenant solution for enterprise businesses called CloudHSM. This gives more control to the client. Now, let’s have a small discussion about the captivating subject of standards in cryptography.\n\nIn 1901 the National Institute of Standards and Technology was founded as a laboratory for promoting innovation and industrial competitiveness for the science and technology sector. Now a part of the U.S. Department of Commerce, these are the folks who set security standards for the stuff we use. In 2002 the E-Government Act was signed into law and then amended in 2014 to include new measures for cybersecurity. This included several plans to beef up encryption standards, among them is the Federal Information Process Standards or FIPS. This program sets legal requirements for U.S. government systems and the systems for any contractors. The reason I bring all this is up is that KMS in its most basic form is compliant with FIPS 140-2 Level 2 compliant. If a client were to use the single-tenant CloudHSM solution for managing keys, then they comply with FIPS 140-2 Level 3. The different levels break down into levels of security through more aggressive security solutions. You can read about all the levels here. A business would only need to worry about having their own CloudHSM if they are trying to comply with a specific regulation. For most of us, a regular KSM solution would work fine.\n\nCustomer Master Keys \n\nAlright, so we know KMS is super cereal.\n\n\n\nNow how does it work? The primary resources in KMS are the Customer Master Keys or CMKs which are logical representations of the master keys. These master keys are used to encrypt and decrypt our data. They use what is called envelope encryption for securing keys and enabling encryption.\n\n\n\nWhen you encrypt your data whatever you encrypted is protected but you also need to protect the encryption keys. Envelope encryption is the practice of encrypting plaintext data with a data key and then encrypting the data key with another key. The data keys are strings of data used to unlock crypto functions like authentication, authorization, and encryption. The role of the master key is to keep the data keys safe. The master keys are what is stored on the HSM and they are used to encrypt all the other keys. The CMKs you make also have metadata attached to them which track key ID, description, creation date, and key state. This metadata also includes the needed material for encryption and decrypting data.\n\nSo, let’s see what we’re talking about.\n\n\n\nThis is the KMS dashboard where you can check out your keys. On the left, you notice that it defaults to the “Customer managed keys” menu where you can create your keys. There is also the option for AWS Managed Keys. These are keys created by AWS for various services that you use. For myself, I have a key for Lambda which is a default key I can use to encrypt environment variables. And a key for the Cloud9 IDE AWS offers. Cloud9 is an awesome cloud-based IDE where you won’t run into issues with Posix permissions for stuff you make. Mini-rant over. The last option is for any “Custom key stores” you might have. Clients using AWS CloudHSM would come here to manage the keys they control within their cluster.\n\nSo, let’s create a key\n\n\n\nAfter we hit the Create Key button we are taken to the Configure Key page. Here we can choose which type of encryption we want. Here we have a couple of options, Symmetric and Asymmetric encryption.\n\nCreating a symmetric key means we are going to create a 256-bit key that uses the same secret key to perform both the encryption and decryption processes. Something like an S4 bucket would be a great candidate for this kind of encryption which uses the AES-256 encryption standard. For me, I created a test key for encrypting environment variables using symmetric encryption.\n\nThe other option is Asymmetric encryption also known as public-key encryption. This will create an RSA key pair used for encryption and decryption or it can also be used for signing and verification, but not both. With an asymmetric key, we create a public key used for encryption and a private key used for decryption. This we can create for something like an EC2 key pair for logging in via SSH to an instance.\n\nThe advanced options let you use a custom store from a CloudHSM a client might own or import a key from an external key management infrastructure. When importing something from an external key management service AWS lays out some rules about how you can’t use KMS to change the key material and how you assume responsibility for some of the things AWS assumes responsibility for when you use KMS to create keys. More information is here.\n\n\n\nHere we can add aliases for the keys. We can also add a description. We can also add tags that we might have created for tracking these assets through our billing set up.\n\n\n\nThe next screen lets us give out permissions for our key. In a large environment, you can expect to assign this key to the needed administrator. That way only that user has permissions to administer the key using the KMS API. Another option you also get is whether to allow admins to delete this key we are making. For any environment, I am making I would probably uncheck this. From my non-cloud sys admin perspective when you have lots of keys distributed around to many different services to have a key deleted for any reason is a recipe for potential nightmare scenarios hunting down where the key was used and redeploying the data since there’s a good chance you’re locked out of getting to it.\n\n\n\nThe next screen lets us give basic permissions to use the CMK in cryptographic operations. So we can choose who can use this key to encrypt and decrypt assets. You can also add additional accounts here if we need another user that might not be on the list.\n\n\n\nFinally, we can review and make changes to the key policy. AWS creates these rules via a JSON file. So, if you know how to navigate through the fields and understand what you are changing you are free to change the policy as you see fit. I would suggest using the GUI to do anything rather than changing the JSON directly. Unless you know what, it is you are doing. Screw around and find out at your peril.\n\n\n\nKMS and Lambda \n\nO.K. we spent a bunch of time talking about a bunch of nerd stuff. I just wanted to encrypt my environment variables. What is all this? We are almost there I promise.\n\n\n\nNow that we have a CMK set up let’s go into Lambda. Maybe we go into a new function that we created to test this out. Then we go into the environment variables. Now we can talk a little about how Lambda works and what impacts there are when we encrypt environment variables. Now Lambda is essentially a service that runs a single instance when called. So, no one sees your environment vars other than when a user is in the console. That said if you are sending API requests with keys and secrets anyone who might be listening on the line might be able to see it. Which is why we do all this stuff. So, before we send API secrets to Lambda lets encrypt it.\n\n\n\nSo, when we go to make our envs we have the option of encrypting them. This will encrypt them in transit and on the console too. If you notice I already created a variable and encrypted it. Under the value, there is just a random string of gibberish to the naked eye but with the key, I can decrypt the value and get the test key value. If we add a new variable, we can check the Enable helpers for encryption in transit under the Encryption configuration section which brings up the Encrypt button. In that section, we also get the option of using the default Lambda key that AWS creates or one of the CMKs we made earlier. I mentioned this before, but Lambda has a default key that you can use for encrypting environment variables. Makes it easy to encrypt environment variables without setting up additional CMKs. I chose to use a CMK to demonstrate KMS but using the Lambda key would probably be fine if you only have a few. When you start to scale, and you need keys for tons of stuff is where I would say using CMKs is a good idea. It gives you a lot more options for tracking and auditing.\n\nAfter hitting the Encrypt button we get a pop up which gives us the execution policy in the form of a JSON readout. In addition to that, we get one of my favorite things AWS gives you, the code to access your keys. I’m lazy and I will always appreciate being given the code to decode rather than digging through the API.\n\n\n\nSo, I’m just going through how to decrypt and utilize your keys within your Lambda function. You can do a bit more with the API and I suggest you check out the reference here. Similar to when you’re looking for your unencrypted env variable you can use the os.environ() function to grab our variable. Accept now we have just a long string of gibberish. Here is where we use the boto3 library to do some stuff.\n\nBoto3 is the AWS SDK for Python. Here we look for the kms service and use the built-in client class for accessing the KMS functions. Then you can see we use the decrypt() function for decrypting the ciphertext we pass into it. The code also imports the b64decode class from the bas64 module which we use to decode the Encrypted var we grabbed. After that, we need to set our EncryptionContext which is a set of non-secret key-value pairs which represent additional authentication data. The options passed in need to match the same context that was used for encrypting the data.\n\nQuickNote\nYou should know that these are for symmetrical CMKs which are called symmetric because they use the same shared key for encryption and decryption. A standard asymmetric key does not support an encryption context. I used symmetric encryption for these keys so that’s what I am going through.\n\nFinally, we can decode the ciphertext into plaintext using the decode() method. After all that we have access to our decrypted key using the Decrypted variable. As mentioned in the pictured code it’s a good idea to put all this somewhere at the beginning outside the function handler. That way we can have universal access to the decrypted key.\n\nQuickNote\nYou might notice that we grab some additional variables using the os.environ() function. These are built-in in runtime environment variables which we can use to access some metadata from within the Lambda function. Check them out here.\n\nYay, now we can do stuff with our environment variables like pass them along to API calls. Congrats. Celebrate somehow.\n\n\n\nConclusion \n\nSo now you know how to protect ya neck and encrypt ya stuff. Now encryption is a big topic. This is just one small sliver that might be relevant to using AWS Lambda. There is a long history related to cryptography. There is this great book called The Code Book by Simon Singh which delves deep into the history and most ancient implementations of cryptography. I highly recommend it for the power nerds among us who like reading about cryptography.\n\nCreating policies that include encrypting your environment variables means better security and peace of mind. Lamba is a powerful engine and I am enjoying using it so far. My Twittterbot has been running for weeks and it’s cringy as hell so I think I’m doing something right. Now I have the tools to encrypt my API keys and secrets.\n\nAdditional Notes \n\nOne other service that you can also check out is the CloudTrail. You can use CloudTrail and attach them to CMKs so that they can be audited. Cloud trail can keep logs of access which can be used in audits. Very useful for clients with a lot of assets to manage.\n\nOne other additional note is for those who use the AWS CLI there are some commands which might come in handy to know. If you are also studying for an AWS Developer certification you will also need to know these.\n\nCommands:\n\naws kms create-key - creates a unique customer-managed CMK in your AWS\naws kms encrypt - encrypts plaintext into ciphertext by using a CMK\naws kms decrypt - decrypts ciphertext that was encrypted by a KMS customer master key\naws kms re-encrypt - decrypts ciphertext and then re-encrypts it with KMS\naws kms enable-key-rotation - enables automatic rotation of the key material for the specified symmetric CMK. This cannot be done on a CML made by another account\n\n",
      tags: ["KMS","AWS","Lambda"],
      id: 16
    });
    

    index.add({
      title: "Working with Context in Go",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  Context Interface\n  Context in context\n  Context.Background\n  Context.TODO\n  Context.WithCancel\n  Context.WithDeadline\n  Context.WithTimeout\n  Context.WithValue\n  Conclusion\n\n\nIntroduction \n\nWhen you’re having a breakdown caused by the combination of burnout and existential pain, do you get annoyed that your harried cries into the void go unanswered? Well, I can’t help with that, but I can suggest some methods for timing out calls to external or internal services. I’ve been doing research and playing with some of the standard libraries in Go and one of them I find most useful is the context library. Used to get some control over a system that might be running slowly for whatever reason or to enforce a certain level of quality for service calls this small library is a standard for a reason. For any production level systems to keep good flow control the context library is going to be necessary.\n\n\n\nCreated by Sameer Ajmani and introduced in 2014, the context library become a standard library with Go 1.7. If you have looked through some Go library source code you can find tons of examples requiring a context to be passed along. This is just one I’ve used recently. A context is a deadline you can pass into a running process in your code. This deadline can indicate to a process to stop running and return after a condition is met. This becomes useful when reaching out to external APIs, databases as shown above, or system commands.\n\nThe following supposes that the reader knows about goroutines and channels and how they work together. I am going to deep dive into concurrency after writing about context as the context library is part of concurrency. For now, though, goroutines are lightweight threads that can be started for processes and channels are the pipelines used to pass data between these new processes.\n\nContext Interface \n\nThe context library defines a new interface called Context. The Context interface has some interesting fields laid out below:\n\n\n\nThe Deadline field returns the expected time the work is finished and indicates when the context should be canceled.\n\nThe Done field is a channel that is closed when work done for the context should be canceled. This operation can happen asynchronously. The channel can return as nil if the associated context can never be canceled. Different context types will arrange for work to be canceled depending on the circumstances, which we will get into.\n\nErr will return nil until Done is closed. After which Err will either return Canceled if the context was canceled or DealineExceeded if the context’s deadline has passed.\n\nThe Value field is a key-value interface which will return a value associated with the context as a key or nil if there was no value associated. Values should be used carefully as they are not for passing parameters into a function but for request-scoped data transits processes and API boundaries.\n\nContext in context \n\nWhen creating a context in Go it is easy to write out a static context to store and reuse. So far as I can tell from my research this is not the optimal way to work with the context library. Context should take the form needed for each use. It should be shapeless, or in the words of Bruce Lee be like water. Your context should flow through your code and evolve depending on the need.\n\nThere are some exceptions to this. For higher-level processes, you can pass in an empty context when you do not yet have a context in which to pass. These can work as placeholders before being refactored.\n\nContext.Background \n\nThe “Background” function returns an empty non-nil context. There is no associated deadline and no cancelation to speak of. This can be typically used in the main function, for testing, or for creating a top-level context to be made into something else. Looking into the source code you can see that it doesn’t have any logic other than returning an empty context:\n\n\n\nQuickNote:\nTypically, the context is named ctx when it is declared. I’ve seen this in most implementations of context so if you come across ctx in random spots in source code there’s a good chance that it is referring to a context.\n\nContext.TODO \n\nThe TODO function does the same thing. It returns an empty non-nil context. This again is a use case for higher-level functions that may not yet have a function available to use them. In many cases, this would be used as a placeholder when extending your program to use the context library. If you checked out the talk by Sameer Ajmani about the introduction of the context library while refactoring their code at Google they would use the context.TODO to start introducing context into the Google code base without breaking anything.\n\nQuickNote:\nOne thing I will also mention is that somewhere along the way it was suggested that the TODO would be compatible for use in static analysis tools for seeing context propagation across a program. This from what I can tell might have been an off-hand comment from the person who wrote out the notes in the source code. I’ve been looking for the last couple of days and from what I can tell no such tool yet exists. I would investigate how to create such a tool but I’m going to go watch a movie instead.\n\n\n\nContext.WithCancel \n\nLet’s say I’m building a website to review movies. There is a myriad of APIs designed for serving movie information. One of the recent ones I’ve come across is the Studio Ghibli API which is a public API we can just grab stuff from. So, for the special section of the website for Studio Ghibli movies, we’ll use this. The WithCancel function returns a copy of the parent context passed into it with a new Done channel. The new Done channel is closed either when the cancel function is called or when the parent context’s Done channel is closed. Whichever event happens first.\n\nBelow is an example in action:\n\n\n\nHere we are going to simulate a process that is hanging up using the longRunningProcess function. In this example, the function is screwing up but we must run it before we request the JSON data from the API. The “longRunningProcess* function will return an error that will cause the cancel() function within the context to fire.\n\nFor the ghibliReq function we will set up a simple HTTP request using the API and pass a string for locating stuff from the API. Once we set up the request, we have a case statement which will receive channel data. Depending on what happens first the select statement will be sent either the current time or the “Done” channel from the passed in context. If the Done channel is closed we error out, if not we will return the status code from our request.\n\nOur main code starts with setting up the context with a new Background() context which is then passed into a WithCancel() context. The new ctx was passed in an empty context so nothing has happened yet. We then create a new goroutine to create a new thread and call our longRunningProcess. Once that is called we check for errors, which will return since we engineered it that way, and if there are errors we can call the cancel() function in our context. Finally, we use our context to call our request. After we run this we find that the request errored out since it took too long and the cancel() function was called.\n\nIn this example, we are running our longRunningProcess before our request because that is needed before we call our request. If the function errors out we need to be able to call “cancel()” so that we can error out the ghibliReq() function. The way we set it up we are calling cancel for our context before the function has a chance to run. This is intentional to show how the cancel works. We could easily change the time.Sleep() in longRunningProcess to say 1000 milliseconds and our request function will run before cancel() is called but in a production environment if the goal is to make sure we maintain the flow of the call stack we would make sure we’re not returning errors and not calling cancel() for this context.\n\nQuickNote:\nKeep in mind that a context-specific call shouldn’t be a blocking action unless necessary. It’s all about keeping stuff running.\n\nContext.WithDeadline \n\nThe WithDeadline function requires two arguments. One is the parent context and the other is a new time object. The function will take the parent context and adjust it to meet the new time object which was passed in. There are a couple of caveats. If you pass in a context that is already earlier than the passed in the time object then the source code will pass just return a WithCancel context with the same cancellation requirements as the parent which you can see in the source. The Done channel is closed after the new deadline expires. You can also manually return the cancel function or it will close when the parent context’s Done channel is closed. Whichever of those events happens first.\n\nBelow we can go through how the WithDeadline works:\n\n\n\nWe’re going to continue with the idea that we are putting together a movie review site. To be honest it would not be far off character of me to start a website dedicated to talking exclusively about Studio Ghibli movies. The example above is doing something like the withCancel example. We are going to reuse a function to demonstrate our context. Reuse the stuff that works, save yourself some time. We are going to make a request and return the status of said request. The difference is how we handle our context.\n\nHypothetically, we need to create a whole bunch of these cascading requests and we want to make sure that everything is happening on time throughout the call stack. To keep track of time and gracefully error out when needed we can continue to use the deadlines and augment the time for the additional calls. In our example, we create a Background context, then pass that in along with a new time. Now we get a returned context in our ctx variable for about 1 second. In our example, if the request process takes longer than 1 second our context calls the cancel function and closes the Done channel causing the request to error out.\n\nWe can see that this is dependent on the standards that we set. Setting a time implies that you have a decent idea about how long something should take. Which can be dependent on your server availability, internet connection, hardware constraints, etc. I have also seen people grumble about certain service level agreements guaranteeing the return of assets within a certain time frame. With the aim of usability in mind using context, deadlines can help to ensure that we can pull information at a reasonable amount of time and return if not.\n\nContext.WithTimeout \n\nThe next relevant function is the WithTimeout function. This is a slight variation from the WithDeadline function. With a need to make something original in mind the WithTimeout simply returns a WithDeadline context with the time argument passed in added to the deadline. In other words, it acts similar to the WithDeadline in that it will take the parent and augment the time to return a derived context with the new time added to the time before the cancel function is called and the Done channel is closed. I’ll make this example even simpler:\n\n\n\nSame as the example before we set the timeout to close the “Done” channel after the allotted time. In our case, if after a half-second, we’re still waiting for the call we timeout. I love the HTTP go library because it has a built-in function for returning a shadow copy of the request with the new context added.\n\nContext.WithValue \n\nThe last bit of the source I am going to touch on is the ContextWithValue function. This one is a bit controversial since the nature of it, from what I can tell, goes against what the context should be. A context should be a way to ensure that we keep data flowing to and from our programs. The value part of the context though can be used to carry information back and forth. The function allows you to pass in a key-value interface to pass around with your calls.\n\nFrom the original post about context “WithValue provides a way to associate request-scoped values with a context”. I’m going to talk a little about what it shouldn’t be used for. Most articles or tutorials I came across seem to agree that passing information that lives outside of the request itself was a bad idea. DB connections, function arguments, anything that is not created and destroyed within that request is probably not a great design pattern. That said passing values along your context can be useful.\n\nLet’s check out some code:\n\n\n\nWe’re going to use the same code from the last example. Only in this case, we are going to create a new function which will calculate a fake request ID. Say I want to keep a database of all my requests, because… I don’t know, I’m a psychopath. Or I work for the NSA and I’m making some spyware to look in on my ex in the name of national security. And because they don’t train me in operational intelligence, I don’t know how to discern the data that indicates something and white noise, so I collect everything. Even innocuous calls to an open API for looking up animated movie information. I’m very tired right now.\n\nIn our example we do the same as above; set up a context with a timeout for half a second. Only now we have a helper method that will calculate a new request ID and we will use the context to pass that ID along within the context as a new interface that we can access and do stuff with. In this fake scenario, we would log this and close out the context. This will conform to our self-imposed standard of keeping only information relevant to that call. Yay information!\n\nThere is a lot more to be explored about passing along values within a context. I have seen articles where middleware is used to do stuff in between two services to make something work better. I might dig deeper into this and since it’s a bit outside the scope of this I might write about it later. Who knows, I need sleep.\n\nConclusion \n\nThe context library helps to add some sanity to calls in our program. When designing a program incorporating a context in our functions should happen as early as possible. As mentioned, before it is easy to create our function with a TODO as a placeholder and go back when refactoring. It was also mentioned that programs should be created to fail gracefully as well. Take it from someone who spent a long time creating vague fail messages which no one can understand including me. A user should not have to know that a call to something failed just that they aren’t getting their movie title in half a second.\n\nA cool way to picture how useful these contexts can be was touched on in Sameer’s talk. He spoke about the practice of hedged calls where you call out redundant services and take the one which takes less time. It’s all about speed and optimization with them Google people. That is one way in which creating a context to flow through your program would be helpful. When one comes back you cancel out the other which releases the resources that thread might have been using up. The context is a small but very powerful library, it should be used often and with plenty of thought and planning into how it should flow into your program. My hope after reading this is that we all come away with a better understanding of context and how we can use it! If you liked this, had questions and or comments, or you just want to berate me on how much the Last Jedi sucked (it was an imperfect but powerful movie for a world not yet ready for it) hit me up on Twitter! I love topical references.\n",
      tags: ["concurrency","Go"],
      id: 17
    });
    

    index.add({
      title: "Creating a Twitter Bot Using AWS Lambda and Go",
      category: ["Blog"],
      content: "Table Of Contents\n\n  Introduction\n  Twitter App Set-Up\n  Twitter Bot Code\n  Setting up a Lambda Function\n  Conclusion\n\n\nIntroduction \n\nMost people have heard of AWS and developers have started learning how they can use it to further augment the quality of their projects. Recently I have begun the process of becoming one of those people. So far it has been an enlightening deep dive into the different services they offer. It’s hard to get your bearings with something as huge as AWS so for my learning journey I decided to focus on projects I thought would be cool and see how AWS might help facilitate what I build.\n\n\n\nOne project I have been wanting to get into is a Twitter bot using a new language I have been learning, Go. A simple twitter bot is easy to set up. Their API makes it easy to interface with and there are thousands of libraries that utilize this API. I am all for anything which makes my job easier. Let’s see how much easier still I can make this using AWS Lambda.\n\nLambda is a compute service for running code on the cloud. With which you can create functions and triggers to start those functions and Lambda runs your code without having to provision a server. It’s serverless. I realized this is uniquely appropriate for things like a regularly scheduled Twitter bot. With Lambda I do not need to spin up an EC2 or a local VM instance and continually run my bot. I can simply set a schedule and create the bot and Lambda starts up, runs my function, and powers down.\n\nLambda’s free services are also a great use case because the first 1 million requests per month are free. After which you pay $0.20 per additional million requests; The first 400,000 GB seconds are also free and $0.0000166667 for every GB second after that. This Twitter bot is a quote regurgitation service so there is very little chance of it ever exceeding any of that.\n\nTwitter App Set-Up \n\nThe Twitter API has been in v1.1 since 2016. In 2020 however, they began rolling out v2. Rebuilt from the ground up they also overhauled their pricing. For our purposes, we can still use the free tier. Let’s talk about the Twitter end first. If you go to Twitter’s Developer portal you can head to the dashboard and set up a new app. We are assuming here that you have already set up a developer account with Twitter. If you have not, check this out on how to get started.\n\nOnce we get the developer account set up we can head to the Apps portal. Here we can click the top right to create a new app.\n\n\n\nThere is some information required for setting up an app. You need to provide information about the app and a website if the app is going to be attached to one. Once that is set up, we can grab our keys and tokens so and start accessing Twitter through the API library.\n\n\n\nQuick note:\nMake sure to check the permissions for your Twitter App. By default, the permissions are set to read. So, if by the end you see permission errors from the API this might be the culprit. Make sure to set the permissions to the least necessary. Here we are making a bot to post Tweets, so we need “Read and write” permissions. I do not need the ability to DM anyone, so I chose “Read and write”. So, go to the “Keys and tokens” section, copy your consumer key, consumer secret, access token, and access secret. Now that we got our credentials, we are ready to party\n\nTwitter Bot Code \n\nCreating a custom library for dealing with the API is preferable for any production system as it allows for the customization needed. For our purposes, we only need to open a stream and send a tweet through it to post. So, I decided to use the Anaconda Library which still uses v1.1 of the Twitter API.\n\nThe first thing we do is set up our development environment. I use VS Code with the Go plugin to help me. I also use a Windows environment to make everything. Later I will circle back to why this isn’t a great idea. We are also going to use Go modules to keep track of our dependencies. Go 1.11 and up can support Go modules and anything above Go 1.13 is going to use them by default. With that in mind, we set up our file directory as such\n\n\n  Bot\n  go.mod\n\n\nWe CD to the root of our project and run:\n\ngo mod init bot/main\n\n\nNow we have our directory and our mod file which is where our dependencies are housed. Next, we create our bot file.\n\n\n  Bot\n  go.mod\n  bot.go\n  quotes.json\n\n\nYou will also notice that there is a JSON file called quotes.json this is where we will store our quotes for now. Now we can get started building. The first thing is we add our dependencies. I added a few here and I will explain the need for them as we go along.\n\n\n\nThe next part you will see is our data types we created for the bot.\n\n\n\nHere we created structs to house some data. Structs in Go are a way to group related data. I have found them to be some of the most useful data type collections to perform a variety of tasks with. The first named APICred is just a way to house and reuse the API credentials that we grabbed from the Twitter App we created.\n\nThe next structs are for the quote object we will be grabbing from the JSON file. We have a struct made for the individual quotes and one for an array of the aforementioned QuoteObject structs. This makes it easier to grab all our quotes and use the list to set our bot logic. This is not an ideal solution as it assumes we will always grab all the quotes from the list. This is not a scalable solution as we would not want to grab all the entries from say a database and then sort through them manually. However, for this low threshold example, it works.\n\n\n\nQuick Note\nYou will notice for the struct keys that they are all capitalized. This is good practice because for me it looks neater and some issues can come up when working with things like JSON data. When unmarshalling the data into struct fields those fields are only exported if they are capitalized. This allows the JSON package to use the field names to unmarshal the data from the JSON file for use in our bot. I learned this after seeing my data come up blank when grabbing the quotes from the file. You can also see the issue in this Stack Overflow article and save yourself some time.\n\nAnother thing to note on the structs is the tags or annotations attached to the struct keys. In many cases, this might not be necessary. As in the case where everything is named the same on the JSON fields and the struct fields; The package should be smart enough to match them. However, I program as defensively as I can so the JSON tags allow for me to match them up directly. The annotations tell the JSON package that the TweetId field needs to be matched with the tweetid field in the JSON file. The tags do that for all of them. The QuoteList field will grab that entire “quotes” block from the JSON file. This also works when working with binary JSON or BSON information, say from grabbing information from Mongo DB.\n\nHere is the next block of code for facilitating some functions in our bot:\n\n\n\nNow comes some more fun. When working with local files stuff like tokens, keys, DB params, etc we would use a .env file and import it into our code. In this case, we are not using this locally or on a production machine, we are creating an AWS Lambda function! The env variables can be set on the Lambda dashboard. On the code side, we do not need to load an environment library, we can just use the os library to load our environment variables directly from AWS. Above we create a new instance of the APICred and call it env and return that from the function. Here we make use of Go’s return type naming in the function signature to initialize our returned item. Then we set the fields and we are ready to party rock the Twitterverse.\n\nThe next function is a helper function to come up with a random index used to randomly search for a quote to tweet out. We use the math/rand library along with the Seed function to get a random index within the range of quotes that we have. We serve a minimum index value which should always be 0 (maybe make a const next time?) and a max value. In this case, we will count the number of entries (quotes) and use that as the max value. We do this so that we can always come to an index with the range of quotes we have. In other words, if we have ten quotes we want to return an index between 0-10 and use that to match to the tweetid field in the JSON file and grab a quote. The “Seed” function here is important as the rand function is deterministic. As in it will return the same value each time you run it. The “rand” function is considered pseudo-random for this reason. If Seed is not called rand will by default using Seed(1) which will get us the same number each time. By using the Seed function to set the seed on each run and using the Unix time will make sure to feed in an int64 number representing the time that changes every second.\n\nNext, we come up with the logic to grab our quote:\n\n\n\nThis function is used to get a quote and return it as a string to our API object, which then sends the string to Twitter. The first part is to grab the file and return a file object using the os.Open function. This assumed the JSON file is in the same directory as the bot file. If not an error is thrown. We set up the defer quote_file.Close() so that all the other logic in our function runs and the defer runs last. This keeps our file open for us until we are done. This is great for flow control.\n\nNext, we need to convert what is in the files into a []byte list so that we can unmarshal the data. The ioutil.ReadAll function call allows us to assign the contents of the file to a byte object. After that, we unmarshal the data using the json package. Here we set it to unmarshal the quotes_bytes object into a pointer for our quotes object which is initialized in the line above that. Here is where our annotations go to work and make everything work automagically. We set some error handling and then we get to work getting our quote.\n\nWe set the maximum number of quote objects by referencing the QuoteList field and counting the number of objects in the array. Next, we get our random_tweetid which will grab a random index from zero to the max number of quotes. Finally, we get our quote by calling the Quote field from one of the random indexes in our QuoteList which returns a quote ready to be served to Twitter.\n\nThe last part of our bot puts everything together and execute:\n\n\n\nWe load our environment variables using the APICred struct and using the Anaconda library we set our Consumer key and secret and then create the API instance using the access token and secret. Now with all the power at our fingertips, we call our GrabQuote function which grabs a random quote from our JSON file and sets it to the tweet variable. In the PostTweet function call you can see that we pass in our quote via the tweet object and we also pass in url.Value{} which we can call from the net/url library import. If you look through the Anaconda source, in the PostTweet function the url values are used to set some required API fields. Once those are set the library creates a channel to pass in all the fields to send to the API. Diving into how the libraries source is a good idea. You should know how every line in your program works. This includes knowing how a library handles inputs and outputs. We set some error handling, just in case, for the PostTweet and we send the tweet through the API and into the timeline.\n\nEvery Go program has the main function where everything is called. Here we utilize the AWS Lambda for Go library to call our function. The lambda.Start() is how Lambda will call our function. In our case, we house all the logic in the SendTweet function and use the logic in main() to call it. If we were creating say an API we would create a handler function which is called from the lambda.Start() function. In our case I decided not to return anything however, Lambda allows for the return of between 0-2 arguments. This is good for returning logs of the events run and or errors. AWS Lambda offers great tools for logging the actions of your function and the returns are how they are recorded. If setting returns one of the returns must implement an error according to the docs. So you can say return a success log with some information you want to track and an error for when stuff goes wrong. AWS Lambda also allows for passing in between 0-2 function arguments. They do say in the documentation that if passing in an argument a context is required. I have not tested this yet, but I will, and I will probably write about it.\n\nOur bot is done! Let’s party.\n\n\n\nBut Wait, we can’t party yet!\n\n\n\nBefore that, we need to set up a way for our bot to run. It would suck to have to manually run this every time I want to send a quote. Might as well open an alt account and do it manually. It would suck even more to have to provision a cloud instance to run all day for the sake of posting a tweet once or twice a day. Screw that, let’s use Lambda.\n\nSetting up a Lambda Function \n\nHere we are going to assume that we already have an AWS account set up. If you don’t go set it up. It’s useful to have. Now that we’re set up go find your AWS Management Console and search the services for Lambda:\n\n\n\nClick Lambda and you will be brought to the Lambda screen. Here you can find a list for all your applications, functions, and layers. In addition to all that you have a dashboard dedicated to monitoring all your Lambda stuff.\n\n\n\nIt will show the number of functions you have going, the storage used up by your functions, your concurrency, and tons of other stuff. We mentioned the pricing for Lambda; The prices vary depending on the amount you allocate. The pricing also takes into account the duration of time it takes for your functions to execute as well as the number of requests. For us, we are using far less than what is allowed on the free tier since we are only invoking once a day at this point. This will change as I further develop the project.\n\nNow, let’s make a function:\n\n\n\nOn the next screen, we have some information to fill out. The first is authoring options. Here we can leave the default “Author from scratch” option selected. There is also the option for using blueprints to develop our app. If you select this, you are given a list of different already made sample projects which you can use to quickly develop apps. You can also filter out what you are looking for. This makes it easier to quickly prototype an app and start standing up your back end. There is also another option to Browse serverless app repository where you have a similar solution for quickly standing up apps. For our purposes, we are going to author from scratch.\n\nWe put in our function name and select the runtime. The runtime we will circle back to but be sure to select Go 1.x which will set us up to use Golang as our function language. For the permissions, we default to Create a new role with basic Lambda permissions which is fine for now. If you have existing roles created in your IAM console you can feel free to use one. Just be sure that the role has the correct permissions for running Lambda functions. You can also Create a new role from AWS policy templates which allows you to create a role using policy templates built by AWS.\n\nNow we are ready to stand up our function. We are taken to the home page for the function. Here we can start setting up everything:\n\n\n\nThe first thing we are going to do is set our environment variables. As I mentioned above, we are not loading in an env library and we are getting our variables directly from AWS. Here is where we set those up:\n\n\n\nIf you scroll down you will come to a section labeled Environment variables, hit the Edit button and you are taken to the next page where we can set up our vars. If you hit the Add environment variable you can start entering in all your needed variables. In our case, we set up one for the consumer key, consumer secret, access key, and access secret. As noted in our code we will import them in our code using the os.GetEnv function. AWS even notes in the margins how this is done for each language. So spice. AWS lets us set up encryption for our variables as well. This will allow for sensitive information (like API credentials) to be encrypted on the Lambda console and when being used by the API. AWS provides a Key Management API service for encrypting and decrypting your sensitive information. I will not go into how to use the API here but setting up encryption is probably a good idea, so I will write a follow up about it at some point. Once all our variables are set, we can go back to the main page and upload our code.\n\nThe function code is usually available for editing in the “Function Code” section. There a Cloud9 IDE is set up for looking at and editing our code. I have used it and it is a useful tool. I don’t think it is compatible with the Golang runtime though. I will come back to this as I ran into issues when building my function.\n\nNow we have to bundle our code and put it into AWS through our newly made function. As mentioned above I used Windows to create this package so I will go over what I did as reflected in the docs. However, I ran into issues when using Windows to do this. Based on my experimenting and some feedback from some smart people I found that I needed to use a Unix based endpoint to build out the project. I will break down why after.\n\nOnce we have everything set to go on the AWS side, we are ready to build our package. So CMD your way to the root of the package. Now we will set our build environment to Linux using this command:\n\nSet GOOS=linux\n\nWe do this to set the build command to tell the compiler to use the Linux binary. So far as I can tell this is a bit of a known issue with using Windows to do all this. This has not been solved as of the publication of this, but I am sure a fix will come through. Earlier when we set our runtime to Go 1.x we were also setting the server operating system which runs our function. When we say “serverless” we mean someone else’s servers. In this case when setting the Go run time we are setting the OS to run the Amazon Linux operating system. Amazon Linux is a server operating system made to run AWS based services. You can also create images using Amazon Linux for running tons of stuff.\n\nAfter we set the binary, we can build our package:\n\ngo build -o main bot.go\n\n\nIt is important to make sure our executable is called main. As the main function is the invocation point and we are going to set the handler to run main. We need to upload the executable created by the build command to our Lambda function as a zip file. So we send our main executable to a zip file. In our case, we need to zip up the main executable and the quotes.json file since we will need that for our quotes. I did this manually but there is a command you can run for zipping up everything. Shown below:\n\n%USERPROFILE%\\Go\\bin\\build-lambda-zip.exe -output main.zip main\n\n\nQuick Note:\nSo far as I can tell when you zip the contents of the package using Windows it retains Windows permissions. Retention of POSIX permissions is a known issue with using Lambda cross-platform. POSIX permissions are a Unix standard which defines the permission structure used to interact with files and applications. From what I can tell these permissions are not set correctly when doing all this in Windows. However, when I used my Mac (a Unix based OS) I was able to build and test my function without issue.\n\nAfter we zip up our executable and our quotes file we can navigate down to the “Function code” and upload our zip:\n\n\n\nSelect the Upload as zip option and we can upload our newly created zip file.\n\nThe final step is we need to scroll down to the Basic Settings section and select Edit. The edit basic settings page is where we can set a description, change our run time, edit the handler, set the memory, set a time out period, and edit the Execution rule. One thing to note is the memory that you can set to whatever you would like. The default 512 MB is fine, especially in our case. But keep in mind that the more memory the more it may cost. Additionally, you may not need all the memory you set and that could end up costing you money needlessly. For our purposes, it’s fine where it is.\n\n\n\nWe need to edit the handler and set that to main. As I mentioned above, we are using the main handler to invoke our executable within Lambda. So, make sure that when you build your package you name it as main so that the handler will work.\n\nCongrats! Now that we are set up, but wait! We should test this. On the top right of the function home page, there is a drop-down for selecting a test to run. Clicking this gives us the option for setting up a new test. You can use this to set up regular scenarios, edge cases, or any other specific tests you want to run. For us we just need to hit “Configure test events” and this will bring up the test creation page:\n\n\n\nHere we will want to select the Create new test event which will allow us to select an event template. The default Hello World template is fine for now but there are built-in test templates which you can choose from depending on your needs. Once this test is created it can be used as a test template for our function. You can leave the default JSON there, and make sure to name your test event. The JSON can be edited to accommodate whatever you are using. For example, we can test the ability to run a Lambda function via an Alexa trigger. So, the testing JSON file would be where you would pass in echo API session information. For now, the default is fine, when we’re done hit Create. Now we can test our function by hitting the Test button:\n\n\n\nHuzzah! We tested good. Now what? Well, I think we would like to run this bot on a schedule. Lambda allows for a lot of different triggers but for our purposes, we want to run on a regular schedule. Like a cron job. To do this we select the Add trigger button on the Designer section of our function home.\n\nThen we are taken to the trigger page where we want to search for EventBridge (formally CloudWatch Events).\n\n\n\nNext, we hit select the rule for EventBridge:\n\n\n\nWe can reuse old rules we have created or we can hit the Create a new rule option. That opens a bunch of configuration options. We can set the name of the rule, whether it is scheduled or based on an event, and if we want to enable the trigger. We want to schedule this to run daily so we select the Schedule expression option and then enter cron(0 12 * * ? *) as our expression. This is an awesome reference for the cron syntax. This is set to run daily at noon Keep in mind for cron expressions UTC is used. This will run every day at noon UTC. Then we hit save. Now we are done! We have officially added to the background noise of Twitter!\n\nConclusion \n\nDoing this allowed me to check out how cool and simple running stuff off Lambda is. I most likely will continue to do stuff with it as I continue to develop this project. A couple of things I will experiment with is using a Cloud9 IDE available in AWS to develop things like this. It is Unix based so performing the build functions won’t pose too many new issues. It looks as if AWS is bringing the users to the cloud, even by force if needed. It’s not a bad thing though, nothing is perfect, and this is a great solution for running serverless back end services.\n\nThe second part of this series will focus on integrating a database into our Lambda function. Putting six quotes in a JSON file might be an interesting challenge to pull from but it hardly makes for a good way to send out information or useless quotes in our case. I am also going to explore possible language interpretation to create a trained bot that throws out randomly generated quotes based on input data. I’m not looking to replicate the racism, sexism, and pure unadulterated horribleness flying through the Twitter pipes. So, it will be an interesting challenge to try and filter much of that out. We shall see. For now, use Lambda, it’s awesome!\n\nNow we can actually party!\n\n\nFull Github Code\nAWS Docs\n",
      tags: ["AWS","Go"],
      id: 18
    });
    


var store = [{
    "title": "Browser Testing With Cypress",
    "link": "/blog/2022-browser-testing-with-cypress.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2022-07-16-browser-testing-with-cypress/cover.jpg",
    "date": "July 16, 2022",
    "category": ["Blog"],
    "excerpt": "Introduction When I write code, I try to remember that everything is broken until proven otherwise. I’ve been shifting my..."
},{
    "title": "Adventures In TypeScript: Typing My Way Out of Problems",
    "link": "/blog/adventures-in-typescript-typing-my-way-out-of-problems.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2022-06-19-adventures-in-typescript-typing-my-way-out-of-problems/cover.jpg",
    "date": "June 19, 2022",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction The Problem Rendering A Link Objects And Typing Issues Cheating Another Solution The Better Solution Introduction..."
},{
    "title": "Adventures In TypeScript: Destructuring and Code Organization",
    "link": "/blog/adventures-in-typescript-destructuring-and-code-organization.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2022-06-11-adventures-in-typescript-destructuring-and-code-organization/cover.png",
    "date": "June 11, 2022",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Destructuring Destructuring Imports A Cleaner Way To Destructure Imports My Conclusions Introduction I’ve been diving deep..."
},{
    "title": "Adventures In TypeScript",
    "link": "/blog/adventures-in-typescript.html",
    "image": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Typescript_logo_2020.svg/768px-Typescript_logo_2020.svg.png?20210506173343",
    "date": "May 22, 2022",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction TypeScript So Far Type Definitions In Functions Generics Union Types Conclusion Introduction I started a new..."
},{
    "title": "Context In React",
    "link": "/blog/context-in-react.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2022-04-18-context-in-react/cover.png",
    "date": "April 18, 2022",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Data In React Context In React Context In Use Conclusion Introduction React offers the ability to..."
},{
    "title": "Spiking Tailwind CSS in a React App",
    "link": "/blog/spiking-tailwind-in-a-react-app.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2022-04-02-spiking-tailwind-in-a-react-app/cover.png",
    "date": "April 2, 2022",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Tailwind Setup Using Tailwind in React Something a Little More Introduction It’s been some time since..."
},{
    "title": "Using SWR for Easy API Calls in React",
    "link": "/blog/using-swr-for-easy-api-calls-in-react.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2022-03-25-using-swr-for-easy-api-calls-in-react/cover.png",
    "date": "March 25, 2022",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction SWR Conclusion Introduction React gives us the ability to create powerful and scalable apps. We need..."
},{
    "title": "Table Resource VS Client in AWS",
    "link": "/blog/table-resource-vs-client-in-aws.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2022-03-19-table-resource-vs-client-in-aws/cover.png",
    "date": "March 19, 2022",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction The Problem The Answer The Answer Introduction DynamoDB provides an excellent way to store data in..."
},{
    "title": "Modules in React",
    "link": "/blog/modules-in-react.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2022-03-18-modules-in-react/cover.png",
    "date": "March 18, 2022",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction React Modules In-App Example Named Imports vs. Default Imports Conclusion Introduction The organization of your apps..."
},{
    "title": "Implementing Routing in React",
    "link": "/blog/implementing-routing-in-react.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2022-03-12-implementing-routing-in-react/cover.png",
    "date": "March 12, 2022",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction React Router Dom Setup Explainer Conclusion Introduction Implementing routing in React applications can be one of..."
},{
    "title": "Dockerizing System Tests With Selenium",
    "link": "/blog/dockerizing-system-tests-with-selenium.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2022-03-10-dockerizing-system-tests-with-selenium/selenium.png",
    "date": "March 10, 2022",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Selenium WebDriver Setup Running Tests Notes Conclusion Introduction We are spinning up a new application for..."
},{
    "title": "Shared Memory in Docker",
    "link": "/blog/shared-memory-in-docker.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2022-03-05-shared-memory-in-docker/cover.png",
    "date": "March 5, 2022",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Shared Memory Device Use in Docker Conclusion Introduction Recently, I was experimenting with system testing on..."
},{
    "title": "Using Amazon API Gateway with Lambda",
    "link": "/blog/using-api-gateway-with-lambda.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2022-02-09-using-api-gateway-with-lambda/cover.png",
    "date": "February 9, 2022",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Amazon API Gateway Setting up an API Adding the Trigger Setting the Methods Conclusion Introductions I..."
},{
    "title": "Shenanigans with Shaders",
    "link": "/graphics/shinanigans-with-shaders-copy.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2021-11-21-shenanigans-in-shaders/cover.jpg",
    "date": "November 21, 2021",
    "category": ["Graphics"],
    "excerpt": "Table Of Contents Introduction Shaders Setup Shader Code Conclusion For those of you who love rabbit holes, learning graphics programming..."
},{
    "title": "Messaging and Madness: Sending Messages with AMQP and Amazon MQ",
    "link": "/blog/messaging-and-madness-sending-messages-with-amqp-and-amazon-mq-copy.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2021-10-30-messaging-and-madness/title_card.png",
    "date": "October 30, 2021",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction AMQP AMQP and Amazon MQ Serialization Conclusion Introduction How do software systems talk to each other?..."
},{
    "title": "Health Checking S3 and DynamoDB in AWS",
    "link": "/blog/checking-health-in-s3-and-dynamodb.html",
    "image": "https://georgeoffley-blog-images.s3.amazonaws.com/2021-03-03-checking-health-in-s3-and-dynamodb/cover.jpg",
    "date": "March 3, 2021",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Problem S3 Solution DynamoDB Solution Conclusion Introduction A hybrid infrastructure has tons of exciting challenges. Although..."
},{
    "title": "Encrypting Your Environment Variables in Lambda with KMS",
    "link": "/blog/encrypting-your-envioronment-variables-in-lambda-with-kms.html",
    "image": "https://georgeoffley.com/assets/images/encrypt-lambda-envs.jpg",
    "date": "September 4, 2020",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Key Management System in AWS Customer Master Keys KMS and Lambda Conclusion Additional Notes Introduction Do..."
},{
    "title": "Working with Context in Go",
    "link": "/blog/working-with-context-in-go.html",
    "image": "https://georgeoffley.com/assets/images/working-with-context-in-go.jpg",
    "date": "August 17, 2020",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Context Interface Context in context Context.Background Context.TODO Context.WithCancel Context.WithDeadline Context.WithTimeout Context.WithValue Conclusion Introduction When you’re having..."
},{
    "title": "Creating a Twitter Bot Using AWS Lambda and Go",
    "link": "/blog/creating-a-twitter-bot-using-aws-lambda-and-go.html",
    "image": "https://georgeoffley.com/assets/images/twitter-bot-aws-lambda.jpg",
    "date": "August 8, 2020",
    "category": ["Blog"],
    "excerpt": "Table Of Contents Introduction Twitter App Set-Up Twitter Bot Code Setting up a Lambda Function Conclusion Introduction Most people have..."
}]

$(document).ready(function() {
    $('#search-input').on('keyup', function () {
        var resultdiv = $('#results-container');
        if (!resultdiv.is(':visible'))
            resultdiv.show();
        var query = $(this).val();
        var result = index.search(query);
        resultdiv.empty();
        $('.show-results-count').text(result.length + ' Results');
        for (var item in result) {
            var ref = result[item].ref;
            var searchitem = '<li><a href="'+ hostname + store[ref].link+'">'+store[ref].title+'</a></li>';
            resultdiv.append(searchitem);
        }
    });
});